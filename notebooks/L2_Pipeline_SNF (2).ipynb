{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "238ec855-4696-41aa-9737-0fd8dac0cf57",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Snowflake Imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "cc11fd08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Snowflake & Snowpark --\n",
    "from snowflake.snowpark.context import get_active_session\n",
    "from snowflake.snowpark.exceptions import SnowparkSQLException\n",
    "from snowflake.snowpark import Session\n",
    "from snowflake.snowpark.functions import col\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab44072a-92d7-4823-99f1-19ed8cacca0e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### General Imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bb240fc-2276-44bc-8e2d-b14fcf71d13f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Core Python Libs ---\n",
    "import json\n",
    "import re\n",
    "import traceback\n",
    "import os\n",
    "import textwrap\n",
    "import base64\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import threading\n",
    "\n",
    "# --- Third-Party Libs ---\n",
    "import pandas as pd\n",
    "import yaml\n",
    "import requests\n",
    "import sqlparse\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0563a4b5-dea1-4003-bab0-a0bfe5a818c1",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### UI Imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4db66047-0cce-4053-8f5c-c54edc1a661b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Interactive UI Widgets ---\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2222713-09f7-4307-b99d-1bcd3b3355e7",
   "metadata": {},
   "source": [
    "### Global Variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1d208f2a-4b90-46a9-a817-dd30740bb355",
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET_DB =\"DATAPLATR_DEMO\"\n",
    "L1_SCHEMA = \"STG_SI\" \n",
    "L2_SCHEMA = \"DWH_SI\"   \n",
    "\n",
    "MODEL = \"llama3.1-70b\"\n",
    "#\"snowflake-arctic\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7e45b41-709b-45f9-ac2e-fb5a67c8b4b8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Github Configs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "444fdf66-fad4-4cf1-bec8-233118d764b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "git_repo_url=\"https://github.com/poojapapney/snf_elt.git\"\n",
    "GITHUB_REPO = \"poojapapney/snf_elt\"\n",
    "BRANCH=\"PP-Test\"\n",
    "GITHUB_TOKEN=\"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\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fef53c8b-31a3-4899-b6ca-e35e81945c84",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Snowflake Connection Set up:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "df0a4394",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global snf session variable :\n",
    "\n",
    "SNOWFLAKE_SESSION = None\n",
    "\n",
    "\n",
    "def init_snowflake_session(target_db: str, target_schema: str):\n",
    "    \"\"\"\n",
    "    Initializes or retrieves a global Snowflake session.\n",
    "    Works in both Snowflake-native and local environments.\n",
    "    \"\"\"\n",
    "    global SNOWFLAKE_SESSION\n",
    "\n",
    "    if SNOWFLAKE_SESSION is not None:\n",
    "        \n",
    "        print(\" Reusing existing Snowflake session.\")\n",
    "        \n",
    "        return SNOWFLAKE_SESSION\n",
    "\n",
    "    try:\n",
    "        # Try to use active Snowflake Notebook session:\n",
    "        \n",
    "        SNOWFLAKE_SESSION = get_active_session()\n",
    "        print(\"  Successfully connected to active Snowflake session.\")\n",
    "    except Exception: # if not\n",
    "        print(\"  No active session found. Creating a new session for local development...\")\n",
    "\n",
    "        with open(\"configs.json\", \"r\") as f:\n",
    "            connection_parameters = json.load(f)\n",
    "\n",
    "        SNOWFLAKE_SESSION = Session.builder.configs(connection_parameters).create()\n",
    "        print(\"  Successfully created a new local Snowflake session.\")\n",
    "\n",
    "    # Set database and schema context:\n",
    "    \n",
    "    SNOWFLAKE_SESSION.use_database(target_db)\n",
    "    SNOWFLAKE_SESSION.use_schema(target_schema)\n",
    "\n",
    "    print(f\" Session active with DB: {target_db}, Schema: {target_schema}\")\n",
    "    return SNOWFLAKE_SESSION\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "139881fe-8e8d-48d0-b09e-58dba7e10e9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Successfully connected to active Snowflake session.\n",
      " Session active with DB: DATAPLATR_DEMO, Schema: STG_SI\n"
     ]
    }
   ],
   "source": [
    "SNOWFLAKE_SESSION= init_snowflake_session(TARGET_DB, L1_SCHEMA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d890dd47",
   "metadata": {},
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "53bd82c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensure_schema(session, database_name: str, schema_name: str):\n",
    "    \"\"\"Checks if a schema exists and creates it if not.\"\"\"\n",
    "    schema_ref = f'\"{database_name}\".\"{schema_name}\"'\n",
    "    try:\n",
    "        session.sql(f'USE DATABASE \"{database_name}\"').collect()\n",
    "        session.sql(f\"DESC SCHEMA {schema_ref}\").collect()\n",
    "        print(f\"Schema {schema_ref} already exists.\")\n",
    "    except SnowparkSQLException as e:\n",
    "        if \"does not exist\" in str(e).lower():\n",
    "            print(f\"Schema {schema_ref} not found. Creating...\")\n",
    "            session.sql(f\"CREATE SCHEMA IF NOT EXISTS {schema_ref}\").collect()\n",
    "            print(f\"Schema {schema_ref} created successfully.\")\n",
    "        else:\n",
    "            raise\n",
    "\n",
    "def parse_model_json(raw_text: str):\n",
    "    # Remove markdown fences if present\n",
    "    cleaned = re.sub(r\"^```json|```$\", \"\", raw_text.strip(), flags=re.MULTILINE).strip()\n",
    "    return json.loads(cleaned)\n",
    "    \n",
    "\n",
    "def commit_files_to_github(repo: str, branch: str, token: str, files: list, commit_message: str):\n",
    "    \"\"\"Pushes a list of files to a GitHub repository in a single batch commit.\"\"\"\n",
    "    print(\"\\nðŸš€ Starting batch commit to GitHub...\")\n",
    "    headers = {\"Authorization\": f\"token {token}\", \"Accept\": \"application/vnd.github+json\"}\n",
    "    api_base = f\"https://api.github.com/repos/{repo}\"\n",
    "\n",
    "    try:\n",
    "        ref_resp = requests.get(f\"{api_base}/git/refs/heads/{branch}\", headers=headers)\n",
    "        ref_resp.raise_for_status()\n",
    "        commit_sha = ref_resp.json()[\"object\"][\"sha\"]\n",
    "        \n",
    "        commit_obj = requests.get(f\"{api_base}/git/commits/{commit_sha}\", headers=headers)\n",
    "        commit_obj.raise_for_status()\n",
    "        base_tree_sha = commit_obj.json()[\"tree\"][\"sha\"]\n",
    "\n",
    "        tree = [{\"path\": f[\"path\"], \"mode\": \"100644\", \"type\": \"blob\", \"content\": f[\"content\"]} for f in files]\n",
    "\n",
    "        tree_resp = requests.post(\n",
    "            f\"{api_base}/git/trees\", headers=headers, json={\"base_tree\": base_tree_sha, \"tree\": tree}\n",
    "        )\n",
    "        tree_resp.raise_for_status()\n",
    "        new_tree_sha = tree_resp.json()[\"sha\"]\n",
    "\n",
    "        commit_resp = requests.post(\n",
    "            f\"{api_base}/git/commits\",\n",
    "            headers=headers,\n",
    "            json={\"message\": commit_message, \"tree\": new_tree_sha, \"parents\": [commit_sha]},\n",
    "        )\n",
    "        commit_resp.raise_for_status()\n",
    "        new_commit_sha = commit_resp.json()[\"sha\"]\n",
    "\n",
    "        patch_resp = requests.patch(\n",
    "            f\"{api_base}/git/refs/heads/{branch}\", headers=headers, json={\"sha\": new_commit_sha}\n",
    "        )\n",
    "        patch_resp.raise_for_status()\n",
    "\n",
    "        print(f\"Successfully pushed {len(files)} files to {repo}@{branch}\")\n",
    "    except Exception as e:\n",
    "        print(f\"GitHub batch commit failed: {e}\")\n",
    "        if hasattr(e, \"response\") and e.response is not None:\n",
    "            print(f\"    Response body: {e.response.text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fe9ab72",
   "metadata": {},
   "source": [
    "### Silver L2 Agent: LLM Suggestions & User Interface\n",
    "These functions suggest keys using a local LLM call and display an interactive UI for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c23790e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def suggest_l2_keys_snowflake(session: Session, l1_metadata: dict, model_name: str = MODEL) -> dict:\n",
    "    \"\"\"Uses Snowflake Cortex AI_COMPLETE to suggest keys for an L2 incremental model.\"\"\"\n",
    "    print(\"Calling Snowflake Cortex to suggest keys...\")\n",
    "    columns_json = json.dumps(l1_metadata.get(\"columns\", []), indent=2)\n",
    "    \n",
    "    # Sanitize the JSON string for use inside a SQL query\n",
    "    escaped_columns_json = columns_json.replace(\"'\", \"''\")\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "    You are an expert dbt data engineer designing an incremental model (SCD Type 1).\n",
    "    Based on the following columns from a source view, please suggest the best keys.\n",
    "\n",
    "    Columns:\n",
    "    {escaped_columns_json}\n",
    "\n",
    "    Your Task is to suggest:\n",
    "    1.  **unique_key**: A list of one or more columns that uniquely identify a single record (the primary key). This is mandatory.\n",
    "    2.  **non_null_fields**: A list of columns that should never be null (e.g., important business fields). # <-- ADDED\n",
    "    3.  **incremental_key**: The single best column for incremental loading. This must be a timestamp representing the last modification date.\n",
    "    4.  **delete_flag**: A boolean or status column that indicates if a record has been soft-deleted (e.g., 'IS_DELETED').\n",
    "\n",
    "    Respond ONLY in valid JSON format like this, with no extra text or markdown:\n",
    "    {{\n",
    "      \"unique_key\": [\"COLUMN_NAME_1\"],\n",
    "      \"non_null_fields\": [\"COLUMN_NAME_1\", \"ANOTHER_IMPORTANT_COLUMN\"],\n",
    "      \"incremental_key\": \"LAST_MODIFIED_DATE\",\n",
    "      \"delete_flag\": \"IS_DELETED\"\n",
    "    }}\n",
    "    \"\"\"\n",
    "    \n",
    "    # Escape single quotes in the prompt for the SQL query\n",
    "    escaped_prompt = prompt.replace(\"'\", \"''\")\n",
    "\n",
    "    query = f\"\"\"\n",
    "    SELECT AI_COMPLETE('{model_name}', '{escaped_prompt}',\n",
    "         OBJECT_CONSTRUCT('temperature', 0))\n",
    "    \"\"\"\n",
    "    \n",
    "    result = \"\"\n",
    "    try:\n",
    "        result = session.sql(query).collect()[0][0]\n",
    "        \n",
    "        # First parse attempt\n",
    "        suggestions = json.loads(result)\n",
    "        \n",
    "       \n",
    "        if isinstance(suggestions, str):\n",
    "            suggestions = json.loads(suggestions)\n",
    "\n",
    "        print(\"Cortex suggestions received.\")\n",
    "        return suggestions\n",
    "        \n",
    "    except (json.JSONDecodeError, IndexError) as e:\n",
    "        print(f\" Cortex did not return valid JSON. Error: {e}\\\\nRaw Response: {result}\")\n",
    "        return {} # Return empty dict on failure\n",
    "\n",
    "def interactive_key_selection_ui(columns: list, suggestions: dict, table_name: str, callback):\n",
    "    \"\"\"Displays an interactive UI with a title for a user to confirm or edit LLM-suggested keys.\"\"\"\n",
    "    \n",
    "    def_val = lambda key, fallback=None: suggestions.get(key, fallback)\n",
    "\n",
    "    \n",
    "    title_widget = widgets.HTML(f\"<h3>Select Keys for: {table_name}</h3>\")\n",
    "    \n",
    "\n",
    "    unique_keys_widget = widgets.SelectMultiple(\n",
    "        options=columns, description=\"Unique Keys:\",\n",
    "        value=[v for v in def_val(\"unique_key\", []) if v in columns],\n",
    "        layout=widgets.Layout(width=\"90%\", height=\"100px\")\n",
    "    )\n",
    "    \n",
    "    # <-- ADDED THIS WIDGET -->\n",
    "    non_null_fields_widget = widgets.SelectMultiple(\n",
    "        options=columns, description=\"Non-Null Fields:\",\n",
    "        value=[v for v in def_val(\"non_null_fields\", []) if v in columns],\n",
    "        layout=widgets.Layout(width=\"90%\", height=\"100px\")\n",
    "    )\n",
    "    \n",
    "    incremental_key_widget = widgets.Dropdown(\n",
    "        options=columns, description=\"Incremental Key:\",\n",
    "        value=def_val(\"incremental_key\") if def_val(\"incremental_key\") in columns else None,\n",
    "        layout=widgets.Layout(width=\"90%\")\n",
    "    )\n",
    "    \n",
    "    delete_flag_widget = widgets.Dropdown(\n",
    "        options=[\"None\"] + columns, description=\"Delete Flag:\",\n",
    "        value=def_val(\"delete_flag\") if def_val(\"delete_flag\") in columns else \"None\",\n",
    "        layout=widgets.Layout(width=\"90%\")\n",
    "    )\n",
    "\n",
    "    submit_btn = widgets.Button(description=\"Confirm and Process Next\", button_style='success', layout=widgets.Layout(width=\"50%\"))\n",
    "    \n",
    "    \n",
    "    ui = widgets.VBox([\n",
    "        title_widget, \n",
    "        unique_keys_widget, \n",
    "        non_null_fields_widget, \n",
    "        incremental_key_widget, \n",
    "        delete_flag_widget, \n",
    "        submit_btn\n",
    "    ])\n",
    "    \n",
    "    display(ui)\n",
    "\n",
    "    def on_submit(b):\n",
    "        selections = {\n",
    "            \"unique_key\": list(unique_keys_widget.value),\n",
    "            \"non_null_fields\": list(non_null_fields_widget.value), # <-- ADDED\n",
    "            \"incremental_key\": incremental_key_widget.value,\n",
    "            \"delete_flag\": delete_flag_widget.value if delete_flag_widget.value != \"None\" else None\n",
    "        }\n",
    "        \n",
    "        callback(selections)\n",
    "\n",
    "    submit_btn.on_click(on_submit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "fc7580ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_l2_dbt_artifacts(l1_view_name: str, l2_table_name: str, selections: dict, metadata: dict) -> dict:\n",
    "    \n",
    "    \"\"\"Generates the content for the dbt .sql and .yml files for an L2 incremental model.\"\"\"\n",
    "\n",
    "    # === Extract selections ===\n",
    "    unique_key = selections['unique_key']\n",
    "    incremental_key = selections['incremental_key']\n",
    "    delete_flag = selections.get('delete_flag') \n",
    "    non_null_fields = selections.get('non_null_fields', [])\n",
    "\n",
    "    if not unique_key or not incremental_key:\n",
    "        raise ValueError(\"Unique Key and Incremental Key are mandatory selections.\")\n",
    "\n",
    "    # === 1. LOGIC FOR SOFT DELETES ===\n",
    "    post_hook_config = \"\"\n",
    "    if delete_flag:\n",
    "        post_hook_config = f\"\"\"\n",
    "        , post_hook=[\n",
    "            \"DELETE FROM {{{{ this }}}} WHERE {delete_flag} = TRUE\"\n",
    "        ]\n",
    "        \"\"\"\n",
    "\n",
    "    # === 2. LOGIC FOR SOURCE DEDUPLICATION & INCREMENTAL LOAD ===\n",
    "    partition_by_str = \", \".join(unique_key)\n",
    "    \n",
    "    # --- Generate SQL Content ---\n",
    "    sql_content = textwrap.dedent(f\"\"\"\n",
    "    {{{{\n",
    "        config(\n",
    "            materialized='incremental',\n",
    "            incremental_strategy='merge',\n",
    "            merge_exclude_columns = ['DW_INSERT_DATE'],\n",
    "            schema='{L2_SCHEMA}',\n",
    "            unique_key={unique_key}{post_hook_config}\n",
    "        )\n",
    "    }}}}\n",
    "\n",
    "    -- This model incrementally builds the L2 table (SCD Type 1)\n",
    "    -- It merges new records and updates existing ones based on the unique key.\n",
    "\n",
    "    WITH filtered_source AS (\n",
    "        SELECT\n",
    "            *, {{{{ generate_audit_column('DW_INSERT_DATE') }}}}\n",
    "             ,{{{{ generate_audit_column('DW_UPDATE_DATE') }}}}\n",
    "        FROM {{{{ ref('{l1_view_name}') }}}}\n",
    "\n",
    "        {{% if is_incremental() %}}\n",
    "        -- This filter is applied *before* deduplication, just like in GCP\n",
    "        WHERE {incremental_key} > (SELECT MAX({incremental_key}) FROM {{{{ this }}}})\n",
    "        {{% endif %}}\n",
    "    )\n",
    "\n",
    "    SELECT\n",
    "        *\n",
    "    FROM filtered_source\n",
    "    \n",
    "    -- Deduplicate *after* filtering for new records\n",
    "    QUALIFY ROW_NUMBER() OVER (\n",
    "      PARTITION BY {partition_by_str}\n",
    "      ORDER BY {incremental_key} DESC\n",
    "    ) = 1\n",
    "    \"\"\")\n",
    "\n",
    "    # --- Generate YAML Content ---\n",
    "    \n",
    "    unique_key_set = set(unique_key)\n",
    "    non_null_set = set(non_null_fields) | unique_key_set \n",
    "\n",
    "    column_yaml = []\n",
    "    for col in metadata.get(\"columns\", []):\n",
    "        col_name = col['name']\n",
    "        \n",
    "        # --- FIX: Replaced \"''\" with '\\\"' for correct YAML escaping ---\n",
    "        # --- CORRECTED: Removed 'or 'No description provided.'' ---\n",
    "        col_desc = (col.get('description') or \"\").replace('\"', '\\\\\"')\n",
    "        \n",
    "        # --- FIX: Changed \\\\n to \\n and fixed quote escaping ---\n",
    "        entry = f\"      - name: {col_name}\\n        description: \\\"{col_desc}\\\"\"\n",
    "        \n",
    "        tests_to_add = []\n",
    "        if col_name in unique_key_set:\n",
    "            tests_to_add.append(\"unique\")\n",
    "        \n",
    "        if col_name in non_null_set:\n",
    "            tests_to_add.append(\"not_null\")\n",
    "\n",
    "        if tests_to_add:\n",
    "            # --- FIX: Changed \\\\n to \\n ---\n",
    "            entry += \"\\n        tests:\"\n",
    "            for test_name in tests_to_add:\n",
    "                entry += f\"\\n          - {test_name}\"\n",
    "        \n",
    "        column_yaml.append(entry)\n",
    "\n",
    "    # --- FIX: Replaced \"''\" with '\\\"' for correct YAML escaping ---\n",
    "    # --- CORRECTED: Removed 'or 'No description provided.'' ---\n",
    "    table_desc = (metadata.get(\"table_description\") or \"\").replace('\"', '\\\\\"')\n",
    "    \n",
    "    # --- FIX: Removed the trailing \\ from f\"\"\"\\ ---\n",
    "    yaml_content = textwrap.dedent(f\"\"\"\n",
    "    version: 2\n",
    "\n",
    "    models:\n",
    "      - name: {l2_table_name}\n",
    "        description: \"{table_desc}\"\n",
    "        columns:\n",
    "    \"\"\") + \"\\n\".join(column_yaml)\n",
    "\n",
    "    return {\n",
    "        \"sql_path\": f\"models/silver/L2/{l2_table_name}.sql\",\n",
    "        \"sql_content\": sql_content,\n",
    "        \"yaml_path\": f\"models/silver/L2/{l2_table_name}.yml\",\n",
    "        \"yaml_content\": yaml_content\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42b5923c",
   "metadata": {},
   "source": [
    "### Main Silver L2 Pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "721a2b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class L2PipelineOrchestrator:\n",
    "    def __init__(self, session, db, l1_schema, l2_schema, github_repo, branch, token):\n",
    "        self.session = session\n",
    "        self.db = db\n",
    "        self.l1_schema = l1_schema\n",
    "        self.l2_schema = l2_schema\n",
    "        self.github_repo = github_repo\n",
    "        self.branch = branch\n",
    "        self.token = token\n",
    "        self.all_generated_files = []\n",
    "        self.l1_view_names = []\n",
    "        self.current_index = 0\n",
    "        self.current_l1_metadata = {}\n",
    "        #\n",
    "        self.output_area = widgets.Output()\n",
    "        \n",
    "\n",
    "    def start(self):\n",
    "        \"\"\"the pipeline process and displays the output area.\"\"\"\n",
    "        \n",
    "        display(self.output_area)\n",
    "        #\n",
    "        \n",
    "        with self.output_area:\n",
    "            ensure_schema(self.session, self.db, self.l2_schema)\n",
    "            \n",
    "            print(f\"\\nFetching L1 views from schema: {self.db}.{self.l1_schema}...\")\n",
    "            l1_views_df = self.session.sql(f\"\"\"\n",
    "                SELECT TABLE_NAME\n",
    "                FROM {self.db}.INFORMATION_SCHEMA.TABLES\n",
    "                WHERE TABLE_SCHEMA = '{self.l1_schema}' \n",
    "                  AND TABLE_TYPE = 'VIEW'\n",
    "                  AND TABLE_NAME LIKE 'V_STG_%_L1' -- <--- ADD THIS LINE\n",
    "                ORDER BY TABLE_NAME\n",
    "            \"\"\").to_pandas()\n",
    "\n",
    "            if l1_views_df.empty:\n",
    "                print(f\" No L1 views found in {self.db}.{self.l1_schema}. Please run the L1 pipeline first.\")\n",
    "                return\n",
    "\n",
    "            self.l1_view_names = l1_views_df['TABLE_NAME'].tolist()\n",
    "            print(f\"Found {len(self.l1_view_names)} L1 views to process: {self.l1_view_names}\")\n",
    "            \n",
    "        self._prepare_next_table()\n",
    "\n",
    "    def _prepare_next_table(self):\n",
    "        \"\"\"Fetches metadata/suggestions and displays UI, logging into the output area.\"\"\"\n",
    "        \n",
    "        \n",
    "        if self.current_index >= len(self.l1_view_names):\n",
    "            with self.output_area:\n",
    "                clear_output(wait=True) \n",
    "                print(\"\\nðŸŽ‰ All tables have been processed locally.\")\n",
    "            self._commit_to_github() # Attempt commit (logs will also go to output_area)\n",
    "            return\n",
    "\n",
    "        \n",
    "        with self.output_area:\n",
    "            clear_output(wait=True) \n",
    "        \n",
    "        l1_view_name = self.l1_view_names[self.current_index]\n",
    "\n",
    "       \n",
    "        with self.output_area:\n",
    "            print(f\"\\n{'='*20}\\nProcessing L1 View ({self.current_index + 1}/{len(self.l1_view_names)}): {l1_view_name}\\n{'='*20}\")\n",
    "\n",
    "            # 1. Fetch L1 metadata from Snowflake\n",
    "            try:\n",
    "                cols_df = self.session.sql(f\"\"\"\n",
    "                    SELECT COLUMN_NAME, COMMENT\n",
    "                    FROM {self.db}.INFORMATION_SCHEMA.COLUMNS\n",
    "                    WHERE TABLE_SCHEMA = '{self.l1_schema}' AND TABLE_NAME = '{l1_view_name}'\n",
    "                    ORDER BY ORDINAL_POSITION\n",
    "                \"\"\").to_pandas()\n",
    "                \n",
    "                table_comment_df = self.session.sql(f\"\"\"\n",
    "                    SELECT COMMENT\n",
    "                    FROM {self.db}.INFORMATION_SCHEMA.TABLES\n",
    "                    WHERE TABLE_SCHEMA = '{self.l1_schema}' AND TABLE_NAME = '{l1_view_name}'\n",
    "                \"\"\").to_pandas()\n",
    "\n",
    "                self.current_l1_metadata = {\n",
    "                    \"table_description\": table_comment_df['COMMENT'][0] if not table_comment_df.empty else \"\",\n",
    "                    \"columns\": [{\"name\": row['COLUMN_NAME'], \"description\": row['COMMENT']} for _, row in cols_df.iterrows()]\n",
    "                }\n",
    "                column_names = [col['name'] for col in self.current_l1_metadata['columns']]\n",
    "\n",
    "                # 2. Get LLM suggestions using Snowflake Cortex\n",
    "                suggestions = suggest_l2_keys_snowflake(self.session, self.current_l1_metadata)\n",
    "                \n",
    "                # 3. Display the UI (UI appears below the output area)\n",
    "                print(f\"Waiting for your input for {l1_view_name}...\")\n",
    "                # Pass l1_view_name to the UI function for the title\n",
    "                interactive_key_selection_ui(column_names, suggestions, l1_view_name, self._handle_ui_submission) \n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\" Error preparing table {l1_view_name}: {e}\\n{traceback.format_exc()}\")\n",
    "                print(\"Skipping this table and moving to the next one.\")\n",
    "                self.current_index += 1\n",
    "                \n",
    "                threading.Timer(0.5, self._prepare_next_table).start() \n",
    "\n",
    "    def _handle_ui_submission(self, selections):\n",
    "        \"\"\"Callback: Generates artifacts and triggers next step, logging into the output area.\"\"\"\n",
    "        \n",
    "        # Extra Sanity Checks\n",
    "        if self.current_index >= len(self.l1_view_names):\n",
    "            \n",
    "             with self.output_area:\n",
    "                 print(\"Warning: Submission received after processing should have finished.\")\n",
    "             return \n",
    "       \n",
    "\n",
    "        l1_view_name = self.l1_view_names[self.current_index] \n",
    "        l2_table_name = l1_view_name.replace(\"V_STG_\", \"\").replace(\"_L1\", \"_L2\")\n",
    "        \n",
    "        \n",
    "        with self.output_area:\n",
    "            print(f\" Selections confirmed for {l2_table_name}: {selections}\")\n",
    "            \n",
    "            try:\n",
    "                \n",
    "                artifacts = generate_l2_dbt_artifacts(l1_view_name, l2_table_name, selections, self.current_l1_metadata)\n",
    "                self.all_generated_files.append({\"path\": artifacts[\"sql_path\"], \"content\": artifacts[\"sql_content\"]})\n",
    "                self.all_generated_files.append({\"path\": artifacts[\"yaml_path\"], \"content\": artifacts[\"yaml_content\"]})\n",
    "                print(f\"   Generated {artifacts['sql_path']}\")\n",
    "                print(f\"   Generated {artifacts['yaml_path']}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\" Error generating artifacts for {l2_table_name}: {e}\\n{traceback.format_exc()}\")\n",
    "        \n",
    "        \n",
    "        self.current_index += 1\n",
    "        \n",
    "        \n",
    "        self._prepare_next_table() \n",
    "\n",
    "    def _commit_to_github(self):\n",
    "        \"\"\"Commits all generated files to GitHub, logging into the output area.\"\"\"\n",
    "        with self.output_area: \n",
    "            if self.all_generated_files:\n",
    "                print(f\"\\nAttempting to commit {len(self.all_generated_files)} files to GitHub...\")\n",
    "                commit_message = f\"feat: Add/Update L2 incremental models for schema {self.l1_schema}\"\n",
    "                try: \n",
    "                    # Assuming commit_files_to_github also uses print statements\n",
    "                    commit_files_to_github(self.github_repo, self.branch, self.token, self.all_generated_files, commit_message)\n",
    "                    print(\"GitHub commit process finished.\") \n",
    "                except Exception as e:\n",
    "                    print(f\" An error occurred during the GitHub commit process: {e}\")\n",
    "                    print(traceback.format_exc())\n",
    "            else:\n",
    "                print(\"\\nNo files were generated or an error occurred. Nothing to commit.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "533fad28-1350-4eae-91aa-4ac48771a203",
   "metadata": {},
   "source": [
    "### Execution Workflow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "bd188946",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "430c4b8fb47f4eb8a5e85531822ab469",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create an instance of the orchestrator with configuration\n",
    "orchestrator = L2PipelineOrchestrator(\n",
    "    session=SNOWFLAKE_SESSION,\n",
    "    db=TARGET_DB,\n",
    "    l1_schema=L1_SCHEMA,\n",
    "    l2_schema=L2_SCHEMA,\n",
    "    github_repo=GITHUB_REPO,\n",
    "    branch=BRANCH,\n",
    "    token=GITHUB_TOKEN\n",
    ")\n",
    "\n",
    "# Start the pipeline\n",
    "orchestrator.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd606aea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python snf-env",
   "language": "python",
   "name": "snf_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
