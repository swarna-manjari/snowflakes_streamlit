{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1a15724-5f80-4f94-b8a8-e87345a153dc",
   "metadata": {
    "codeCollapsed": false,
    "language": "python",
    "name": "cell3"
   },
   "outputs": [],
   "source": [
    "from snowflake.snowpark.context import get_active_session\n",
    "from snowflake.snowpark.exceptions import SnowparkSQLException\n",
    "from snowflake.snowpark import Session\n",
    "from snowflake.snowpark.functions import col \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cccb553",
   "metadata": {},
   "outputs": [],
   "source": [
    "session=get_active_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "20914649-f1f1-4f73-bed4-fc2b6054194b",
   "metadata": {
    "codeCollapsed": false,
    "language": "python",
    "name": "cell1"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sqlparse'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjson\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mre\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msqlparse\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtraceback\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'sqlparse'"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "import sqlparse\n",
    "import traceback\n",
    "import os\n",
    "import subprocess\n",
    "import  textwrap\n",
    "\n",
    "import yaml\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "import requests\n",
    "import base64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b50e5f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a260f5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import yaml\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "import requests\n",
    "import base64"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c4125ca-39a5-4d9c-a174-89b8a096d55b",
   "metadata": {
    "collapsed": false,
    "id": "YWbIRpMnSU5O",
    "jupyter": {
     "outputs_hidden": false
    },
    "name": "cell7"
   },
   "source": [
    "### Set Project config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2ce1a8f-c736-48c8-a67a-f96914782084",
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1759936650700,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": -330
    },
    "id": "7wn30y-e1xjK",
    "language": "python",
    "name": "cell8"
   },
   "outputs": [],
   "source": [
    "TARGET_DB =\"DATAPLATR_DEMO\"\n",
    "SRC_SCHEMA=\"SAGEINTACT_SAGEINTACT\"\n",
    "TARGET_SCHEMA=\"STG_SI\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7b63c23-a88a-435d-adfe-b50703888630",
   "metadata": {
    "language": "python",
    "name": "cell2"
   },
   "outputs": [],
   "source": [
    "MODEL = \"snowflake-arctic\"\n",
    "# \"llama3.1-70b\"\n",
    "# \"mistral-7b\" "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0612e168-666a-45ad-9316-e6284e33627f",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "name": "cell5"
   },
   "source": [
    "### Git config:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "681e924a-c7a2-4c07-b607-e58f396dec89",
   "metadata": {
    "language": "python",
    "name": "cell12"
   },
   "outputs": [],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a1705f6-c1fc-420e-b680-a90afc3c805f",
   "metadata": {
    "language": "python",
    "name": "cell26"
   },
   "outputs": [],
   "source": [
    "git_repo_url=\"https://github.com/poojapapney/snf_elt.git\"\n",
    "GITHUB_REPO = \"poojapapney/snf_elt\"\n",
    "BRANCH=\"main\"\n",
    "GITHUB_TOKEN=\"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba43e488-4cd5-4efa-9634-49ab6ff97996",
   "metadata": {
    "codeCollapsed": false,
    "language": "python",
    "name": "cell25"
   },
   "outputs": [],
   "source": [
    "# folders = [\n",
    "#     \"models/generated\",\n",
    "#     \"models/core\",\n",
    "#     \"artifacts/metadata\",\n",
    "#     # \"tests\",\n",
    "#     \"macros\",\n",
    "# ]\n",
    "# for folder in folders:\n",
    "#     os.makedirs(os.path.join(dbt_project_path, folder), exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c18ebcc-5900-4e1e-a196-c26cf3d94925",
   "metadata": {
    "codeCollapsed": false,
    "language": "python",
    "name": "cell4"
   },
   "outputs": [],
   "source": [
    "# import snowflake.connector\n",
    "# from dbt.adapters.snowflake.connections import SnowflakeConnectionManager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9c9329f-d394-48a9-b934-13239bbc311c",
   "metadata": {
    "language": "python",
    "name": "cell10"
   },
   "outputs": [],
   "source": [
    "# snowflake.connector.paramstyle = \"pyformat\"\n",
    "\n",
    "# def patch_for_dbt(session):\n",
    "#     \"\"\"Patch dbt's Snowflake adapter to reuse the active Snowflake notebook session.\"\"\"\n",
    "#     try:\n",
    "#         # get underlying Python connector connection\n",
    "#         existing_conn = session.connection._conn\n",
    "#         existing_conn._paramstyle = \"pyformat\"\n",
    "\n",
    "#         # monkey patch dbt's open() method so it reuses the existing session\n",
    "#         def new_open_method(cls, connection):\n",
    "#             connection.handle = existing_conn\n",
    "#             return connection\n",
    "\n",
    "#         SnowflakeConnectionManager.open = new_open_method\n",
    "#         print(\" Patched dbt Snowflake adapter to use active Snowflake session\")\n",
    "\n",
    "#     except Exception as e:\n",
    "#         print(\" dbt patching failed:\", e)\n",
    "\n",
    "# # Apply patch:\n",
    "\n",
    "# patch_for_dbt(session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a72ee1f4-8a17-4c95-8529-7c17cf05589f",
   "metadata": {
    "codeCollapsed": false,
    "language": "python",
    "name": "cell11"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c814bce8-d386-4997-8c4a-f8f41d8beebc",
   "metadata": {
    "codeCollapsed": false,
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1759936652973,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": -330
    },
    "id": "QjnR5ruvRoKN",
    "language": "python",
    "name": "cell16"
   },
   "outputs": [],
   "source": [
    "def ensure_schema(session, database_name: str, schema_name: str):\n",
    "    \n",
    "    schema_ref = f\"{database_name}.{schema_name}\"\n",
    "\n",
    "    try:\n",
    "        # Set context:\n",
    "        \n",
    "        session.sql(f\"USE DATABASE {database_name}\").collect()\n",
    "        session.sql(f\"DESC SCHEMA {schema_ref}\").collect()\n",
    "        print(f\" Schema {schema_ref} already exists.\")\n",
    "    except SnowparkSQLException as e:\n",
    "        msg = str(e).lower()\n",
    "        if \"does not exist\" in msg or \"not authorized\" in msg or \"invalid\" in msg:\n",
    "            print(f\" Schema {schema_ref} not found. Creating...\")\n",
    "            session.sql(f\"CREATE SCHEMA IF NOT EXISTS {schema_ref}\").collect()\n",
    "            print(f\" Schema {schema_ref} created successfully.\")\n",
    "        else:\n",
    "            raise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6557918b-43f5-49d8-ab8d-aff2503c1492",
   "metadata": {
    "collapsed": false,
    "id": "Mxc6CyLKUqLQ",
    "jupyter": {
     "outputs_hidden": false
    },
    "name": "cell17"
   },
   "source": [
    "### L1 Agent Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b30c719-2298-41af-b88f-03244bb9741f",
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1759936653708,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": -330
    },
    "id": "-eBLxIl-Fu53",
    "language": "python",
    "name": "cell18"
   },
   "outputs": [],
   "source": [
    "def parse_model_json(raw_text: str):\n",
    "    # Remove markdown fences if present\n",
    "    cleaned = re.sub(r\"^```json|```$\", \"\", raw_text.strip(), flags=re.MULTILINE).strip()\n",
    "    return json.loads(cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d9a3c2-abba-4eb7-ba59-672bcb606674",
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1759936653708,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": -330
    },
    "id": "Sg5llWJWcF3g",
    "language": "python",
    "name": "cell20"
   },
   "outputs": [],
   "source": [
    "def call_l1_agent_snowflake(\n",
    "    session: session,\n",
    "    table_ref: str | list,\n",
    "    schema: list,\n",
    "    target_schema: str,\n",
    "    model_name: str = MODEL\n",
    "):\n",
    "    \"\"\"\n",
    "    Generate a standardized SELECT SQL for a given Snowflake table and schema using Cortex AI_COMPLETE.\n",
    "    Produces both SQL and metadata (in JSON) describing the L1 standardized view.\n",
    "    \"\"\"\n",
    "\n",
    "    # Normalize table reference:\n",
    "    \n",
    "    if isinstance(table_ref, list):\n",
    "        db_name, schema_name, table_name = table_ref\n",
    "    elif isinstance(table_ref, str):\n",
    "        db_name, schema_name, table_name = table_ref.split(\".\")\n",
    "    else:\n",
    "        raise TypeError(f\"Invalid type for table_ref: {type(table_ref)}\")\n",
    "\n",
    "    full_table_ref = f\"{db_name}.{schema_name}.{table_name}\"\n",
    "\n",
    "    #  Prepare schema text for prompt :\n",
    "    \n",
    "    schema_text = []\n",
    "    \n",
    "    for col in schema:\n",
    "        if isinstance(col, dict):  # from pandas DF\n",
    "            schema_text.append({\n",
    "                \"name\": col.get(\"name\") or col.get(\"COLUMN_NAME\"),\n",
    "                \"type\": col.get(\"type\") or col.get(\"DATA_TYPE\"),\n",
    "                \"nullable\": col.get(\"nullable\", True),\n",
    "                \"comment\": col.get(\"description\") or col.get(\"COMMENT\") or \"\"\n",
    "            })\n",
    "        else:  # Snowpark Column-like\n",
    "            schema_text.append({\n",
    "                \"name\": getattr(col, \"name\", None),\n",
    "                \"type\": str(getattr(col, \"datatype\", \"\")),\n",
    "                \"nullable\": getattr(col, \"nullable\", True),\n",
    "                \"comment\": getattr(col, \"comment\", \"\")\n",
    "            })\n",
    "\n",
    "    #  LLM Prompt :\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "    You are given a Snowflake table `{full_table_ref}` with schema:\n",
    "    {schema_text}\n",
    "\n",
    "    Task:\n",
    "    - Generate a SELECT statement to standardize the field names and descriptions for a view in schema `{target_schema}`.\n",
    "    - Select all fields strictly from `{full_table_ref}`.\n",
    "    - Follow these naming conventions:\n",
    "        * Use underscores between logical words (e.g., PROJECTID → PROJECT_ID, ACCOUNTNO → ACCOUNT_NO)\n",
    "        * Rename the source fields WHENMODIFIED, WHENCREATED etc as:\n",
    "          WHENMODIFIED AS LAST_MODIFIED_DATE\n",
    "          WHENCREATED AS CREATED_DATE\n",
    "\n",
    "     - Only transform the column names when met with the scenarios like mentioned here and keep the rest of the fields as is.\n",
    "        \n",
    "    - Respond ONLY in valid JSON format like this:\n",
    "      {{\n",
    "        \"sql\": \"SELECT ... FROM {full_table_ref}\"\n",
    "      }}\n",
    "    \"\"\"\n",
    "\n",
    "    #  Call Cortex model :\n",
    "    # query = f\"SELECT SNOWFLAKE.CORTEX.COMPLETE('{model_name}', '{prompt}')\"\n",
    "    \n",
    "    query = f\"\"\" SELECT AI_COMPLETE('{model_name}', $${prompt}$$,\n",
    "         OBJECT_CONSTRUCT(\n",
    "        'temperature', 0,\n",
    "        'max_tokens', 8192\n",
    "    ))\"\"\"\n",
    "    \n",
    "    result = session.sql(query).collect()[0][0]\n",
    "\n",
    "    # Normalize model output :\n",
    "    \n",
    "    raw_output = str(result).strip()\n",
    "    print(f\" Raw model output for {table_name}:\\n{raw_output[:500]}\\n\")\n",
    "\n",
    "    sql = None\n",
    "\n",
    "    # Try parsing JSON safely (including double-encoded):\n",
    "    \n",
    "    try:\n",
    "        parsed = json.loads(raw_output)\n",
    "\n",
    "        # If the parsed result is a string (double-encoded JSON), decode again\n",
    "        if isinstance(parsed, str):\n",
    "            try:\n",
    "                parsed = json.loads(parsed)\n",
    "            except json.JSONDecodeError:\n",
    "                pass\n",
    "\n",
    "        # Finally, if it’s a dict, extract SQL\n",
    "        if isinstance(parsed, dict) and \"sql\" in parsed:\n",
    "            sql = parsed[\"sql\"].strip()\n",
    "\n",
    "    except json.JSONDecodeError:\n",
    "        # Fallback: sometimes model outputs raw SQL text, not JSON\n",
    "        print(\" JSON parsing failed — trying fallback SQL extraction\")\n",
    "        pass\n",
    "\n",
    "    # Fallback clean-up (if no SQL yet):\n",
    "    \n",
    "    if not sql:\n",
    "        cleaned = (\n",
    "            raw_output.replace(\"\\\\n\", \" \")\n",
    "                      .replace('\\\\\"', '\"')\n",
    "                      .replace(\"\\\\\", \"\")\n",
    "                      .strip()\n",
    "        )\n",
    "        if cleaned.upper().startswith(\"SELECT\"):\n",
    "            sql = cleaned\n",
    "        else:\n",
    "            raise ValueError(f\"Model output did not produce valid SQL:\\n{raw_output}\")\n",
    "\n",
    "\n",
    "    #  Validate SQL output :\n",
    "    \n",
    "    if not sql or not sql.upper().startswith(\"SELECT\"):\n",
    "        raise ValueError(f\"Model output did not produce valid SQL:\\n{sql}\")\n",
    "\n",
    "    # Format SQL for readability :\n",
    "    \n",
    "    try:\n",
    "        formatted_sql = sqlparse.format(sql, reindent=True, keyword_case=\"upper\")\n",
    "    except Exception:\n",
    "        formatted_sql = sql\n",
    "\n",
    "    #  Extract aliases (for metadata):\n",
    "    \n",
    "    alias_pattern = re.compile(r'(\\w+)\\s+AS\\s+(\\w+)', re.IGNORECASE)\n",
    "    alias_map = {m.group(1).upper(): m.group(2) for m in alias_pattern.finditer(formatted_sql)}\n",
    "\n",
    "    final_columns = []\n",
    "    for field in schema_text:\n",
    "        orig_name = field[\"name\"].upper()\n",
    "        final_name = alias_map.get(orig_name, orig_name)\n",
    "        final_columns.append({\n",
    "            \"name\": final_name,\n",
    "            \"description\": (field.get(\"comment\") or \"\").replace(\"\\n\", \" \").strip()\n",
    "        })\n",
    "\n",
    "    #  Prepare metadata JSON :\n",
    "    \n",
    "    l1_view = f\"V_STG_{table_name.removeprefix('SRC_')}_L1\"\n",
    "    # table_info = session.sql(f\"SHOW TABLES LIKE '{table_name}' IN {db_name}.{schema_name}\").collect()\n",
    "    table_info = session.sql(f\"\"\"\n",
    "                                SELECT COMMENT AS comment\n",
    "                                FROM {db_name}.INFORMATION_SCHEMA.TABLES\n",
    "                                WHERE TABLE_SCHEMA = '{schema_name}'\n",
    "                                  AND TABLE_NAME = '{table_name}'\n",
    "                            \"\"\").collect()\n",
    "\n",
    "    # table_comment = table_info[0].get(\"comment\", \"\") if table_info else \"\"\n",
    "    # table_comment = table_info[0].asDict().get(\"comment\", \"\") if table_info else \"\"\n",
    "    table_comment = table_info[0][\"COMMENT\"] if table_info else \"\"\n",
    "\n",
    "\n",
    "\n",
    "    metadata = {\n",
    "        \"name\": l1_view,\n",
    "        \"table_description\": table_comment,\n",
    "        \"columns\": final_columns\n",
    "    }\n",
    "\n",
    "    metadata_json = json.dumps(metadata, indent=2, ensure_ascii=False)\n",
    "\n",
    "    # print(f\" Generated L1 SQL for `{l1_view}`\\n\")\n",
    "    # print(formatted_sql)\n",
    "    print(\"\\n Metadata:\")\n",
    "    print(metadata_json)\n",
    "\n",
    "    return formatted_sql, metadata_json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50640fc4-f0fc-427f-b0af-74e9be30b6f3",
   "metadata": {
    "collapsed": false,
    "id": "sJOARmoQiIxG",
    "jupyter": {
     "outputs_hidden": false
    },
    "name": "cell21"
   },
   "source": [
    "### Validate the sql:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4962c7e-8ccb-40a2-adf6-445da1f81471",
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1759936654528,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": -330
    },
    "id": "iM4Z44tYiG8j",
    "language": "python",
    "name": "cell22"
   },
   "outputs": [],
   "source": [
    "def validate_sql_snowflake(session, sql: str):\n",
    "    try:\n",
    "        session.sql(f\"EXPLAIN USING TEXT {sql}\").collect()\n",
    "        print(\" SQL is valid.\")\n",
    "    except Exception as e:\n",
    "        print(\" SQL validation failed:\", e)\n",
    "        print(\"=== SQL Content ===\")\n",
    "        print(sql)\n",
    "        raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe1a6f4e-2959-48e0-bfc1-5d14c80271fe",
   "metadata": {
    "collapsed": false,
    "id": "2Y-hyrb4m7YB",
    "jupyter": {
     "outputs_hidden": false
    },
    "name": "cell27"
   },
   "source": [
    "### View Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba67ce97-c6a7-4fb5-855e-74de9cae1145",
   "metadata": {},
   "outputs": [],
   "source": [
    "def push_file_to_github(repo, file_path, content, token, commit_message, branch=\"main\"):\n",
    "    \n",
    "    \"\"\"Pushes a file to a GitHub repository using the GitHub API.\"\"\"\n",
    "    \n",
    "    api_url = f\"https://api.github.com/repos/{repo}/contents/{file_path}\"\n",
    "    \n",
    "    headers = {\"Authorization\": f\"token {token}\", \"Accept\": \"application/vnd.github.v3+json\"}\n",
    "\n",
    "    sha = None\n",
    "    try:\n",
    "        resp = requests.get(api_url, headers=headers, params={\"ref\": branch})\n",
    "        if resp.status_code == 200:\n",
    "            sha = resp.json().get(\"sha\")\n",
    "            print(f\"   - File '{file_path}' exists. Preparing to update.\")\n",
    "        elif resp.status_code == 404:\n",
    "            print(f\"   - File '{file_path}' not found. Preparing to create.\")\n",
    "        else:\n",
    "            resp.raise_for_status()\n",
    "    except requests.RequestException as e:\n",
    "        print(f\"   - Network error checking for file: {e}\")\n",
    "\n",
    "    payload = {\n",
    "        \"message\": commit_message,\n",
    "        \"content\": base64.b64encode(content.encode('utf-8')).decode('utf-8'),\n",
    "        \"branch\": branch,\n",
    "    }\n",
    "    if sha:\n",
    "        payload[\"sha\"] = sha\n",
    "\n",
    "    response = requests.put(api_url, headers=headers, json=payload)\n",
    "    if response.status_code not in [200, 201]:\n",
    "        raise Exception(f\"GitHub push failed for {file_path}: {response.status_code} - {response.text}\")\n",
    "    \n",
    "    status = \"created\" if response.status_code == 201 else \"updated\"\n",
    "    print(f\" Successfully {status} '{file_path}' in GitHub.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ba15258-95ed-439e-bd46-af5fb493e57d",
   "metadata": {
    "collapsed": false,
    "id": "QN3ASRiEmSeq",
    "jupyter": {
     "outputs_hidden": false
    },
    "name": "cell32"
   },
   "source": [
    "#### Process a single table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4127d3ce-a7aa-4959-84f0-4ca4fc8e0417",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4341e44e-c9cd-4213-8706-63513dcddfb1",
   "metadata": {
    "language": "python",
    "name": "cell24"
   },
   "outputs": [],
   "source": [
    "def create_standardized_view_dbt(\n",
    "    session,\n",
    "    source_db: str,\n",
    "    source_schema: str,\n",
    "    source_table: str,\n",
    "    target_schema: str,\n",
    "    model: str,\n",
    "    github_repo: str,             \n",
    "    github_token: str,\n",
    "    branch: str = \"main\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Generates standardized dbt SQL + metadata YAML and pushes them directly to GitHub.\n",
    "   \n",
    "    \"\"\"\n",
    "\n",
    "    # Skip tables that don't follow naming convention:\n",
    "    \n",
    "    if not source_table.startswith(\"SRC_\"):\n",
    "        print(f\"Skipping {source_table}, must start with 'SRC_'.\")\n",
    "        return\n",
    "\n",
    "    src_table_fq = f\"{source_db}.{source_schema}.{source_table}\"\n",
    "    view_name = f\"V_STG_{source_table[len('SRC_'):] }_L1\"\n",
    "\n",
    "    # Get schema info:\n",
    "    \n",
    "    cols_df = session.sql(f\"\"\"\n",
    "        SELECT COLUMN_NAME, DATA_TYPE, COMMENT\n",
    "        FROM {source_db}.INFORMATION_SCHEMA.COLUMNS\n",
    "        WHERE TABLE_SCHEMA = '{source_schema}'\n",
    "          AND TABLE_NAME = '{source_table}'\n",
    "        ORDER BY ORDINAL_POSITION\n",
    "    \"\"\").to_pandas()\n",
    "\n",
    "    schema_text = [\n",
    "        {\"name\": r.COLUMN_NAME, \"type\": r.DATA_TYPE, \"description\": r.COMMENT or \"\"}\n",
    "        for _, r in cols_df.iterrows()\n",
    "    ]\n",
    "\n",
    "    # Generate SQL and metadata from LLM:\n",
    "    \n",
    "    sql, metadata_json = call_l1_agent_snowflake(session, src_table_fq, schema_text, target_schema, model)\n",
    "    metadata = json.loads(metadata_json)\n",
    "\n",
    "    # Validate SQL:\n",
    "    \n",
    "    validate_sql_snowflake(session, sql)\n",
    "\n",
    "    # Prepare SQL content:\n",
    "    \n",
    "    sql_content = textwrap.dedent(f\"\"\"\\\n",
    "    -- Auto-generated by L1 Agent\n",
    "    -- Source: {src_table_fq}\n",
    "    -- Target Schema: {target_schema}\n",
    "\n",
    "    {{% set target_schema = '{target_schema}' %}}\n",
    "\n",
    "    {sql}\n",
    "    \"\"\")\n",
    "\n",
    "    # Prepare YAML metadata:\n",
    "    \n",
    "    column_entries = \"\\n\".join(\n",
    "        [f\"      - name: {col['name']}\\n        description: \\\"{col.get('description', '')}\\\"\"\n",
    "         for col in metadata[\"columns\"]]\n",
    "    )\n",
    "    yaml_content = textwrap.dedent(f\"\"\"\\\n",
    "    version: 2\n",
    "\n",
    "    models:\n",
    "      - name: {view_name}\n",
    "        description: \"{metadata.get('table_description', '')}\"\n",
    "        columns:\n",
    "    {column_entries}\n",
    "    \"\"\")\n",
    "\n",
    "    # File paths within repo:\n",
    "    \n",
    "    sql_path = f\"models/generated/{view_name}.sql\"\n",
    "    yaml_path = f\"models/generated/{view_name}.yml\"\n",
    "\n",
    "    # # Push to GitHub:\n",
    "    \n",
    "    # push_file_to_github(github_repo, sql_path, sql_content,\n",
    "    #                     f\"Add SQL model for {view_name}\", github_token, branch)\n",
    "    # push_file_to_github(github_repo, yaml_path, yaml_content,\n",
    "    #                     f\"Add metadata for {view_name}\", github_token, branch)\n",
    "\n",
    "    # print(f\" Uploaded {view_name}.sql and {view_name}.yml to {github_repo}@{branch}\")\n",
    "    return sql_path, yaml_path\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "80ab5cc3-2519-4897-b3e7-439c9e276da3",
   "metadata": {
    "language": "python",
    "name": "cell29"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import textwrap\n",
    "import requests\n",
    "\n",
    "def create_all_standardized_views_dbt(\n",
    "    session,\n",
    "    source_db: str,\n",
    "    source_schema: str,\n",
    "    target_schema: str,\n",
    "    model: str,\n",
    "    github_repo: str,\n",
    "    github_token: str,\n",
    "    branch: str = \"main\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Generates standardized dbt models for all SRC_ tables and commits them\n",
    "    to GitHub in a single batch.\n",
    "    \"\"\"\n",
    "\n",
    "    #  Fetch all SRC_ tables:\n",
    "    \n",
    "    tables_df = session.sql(f\"\"\"\n",
    "        SELECT TABLE_NAME\n",
    "        FROM {source_db}.INFORMATION_SCHEMA.TABLES\n",
    "        WHERE TABLE_SCHEMA = '{source_schema}'\n",
    "          AND TABLE_TYPE = 'BASE TABLE'\n",
    "    \"\"\").to_pandas()\n",
    "\n",
    "    if tables_df.empty:\n",
    "        print(f\"No tables found in {source_db}.{source_schema}\")\n",
    "        return\n",
    "\n",
    "    all_files = []\n",
    "\n",
    "    # Generate SQL + YAML for each table:\n",
    "    \n",
    "    for _, row in tables_df.iterrows():\n",
    "        source_table = row[\"TABLE_NAME\"]\n",
    "\n",
    "        if not source_table.startswith(\"SRC_\"):\n",
    "            print(f\"Skipping {source_table} (not an SRC_ table)\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            print(f\" Generating DBT model for {source_table}...\")\n",
    "            sql_path, yaml_path = create_standardized_view_dbt(\n",
    "                session,\n",
    "                source_db,\n",
    "                source_schema,\n",
    "                source_table,\n",
    "                target_schema,\n",
    "                model,\n",
    "                github_repo,\n",
    "                github_token,\n",
    "                branch\n",
    "            )\n",
    "\n",
    "            # Recreate content from the same logic:\n",
    "            \n",
    "            view_name = f\"V_STG_{source_table[len('SRC_'):] }_L1\"\n",
    "            sql_content = textwrap.dedent(f\"\"\"\\\n",
    "            -- Auto-generated by L1 Agent\n",
    "            -- Source: {source_db}.{source_schema}.{source_table}\n",
    "            -- Target Schema: {target_schema}\n",
    "\n",
    "            {{% set target_schema = '{target_schema}' %}}\n",
    "\n",
    "            select * from {source_db}.{source_schema}.{source_table};\n",
    "            \"\"\")\n",
    "            yaml_content = textwrap.dedent(f\"\"\"\\\n",
    "            version: 2\n",
    "\n",
    "            models:\n",
    "              - name: {view_name}\n",
    "                description: \"Auto-generated model for {source_table}\"\n",
    "                columns:\n",
    "                  - name: *\n",
    "                    description: \"All columns\"\n",
    "            \"\"\")\n",
    "\n",
    "            all_files.append({\n",
    "                \"sql_path\": sql_path,\n",
    "                \"yaml_path\": yaml_path,\n",
    "                \"sql_content\": sql_content,\n",
    "                \"yaml_content\": yaml_content\n",
    "            })\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\" Failed for {source_table}: {e}\")\n",
    "\n",
    "    if not all_files:\n",
    "        print(\"No SRC_ tables processed. Exiting.\")\n",
    "        return\n",
    "\n",
    "    print(f\" Prepared {len(all_files)} models. Starting GitHub commit...\")\n",
    "\n",
    "    #  Single GitHub Commit:\n",
    "    \n",
    "    headers = {\"Authorization\": f\"token {github_token}\"}\n",
    "    api_base = f\"https://api.github.com/repos/{github_repo}\"\n",
    "\n",
    "    # Get latest commit on branch:\n",
    "    \n",
    "    branch_info = requests.get(f\"{api_base}/git/refs/heads/{branch}\", headers=headers).json()\n",
    "    base_sha = branch_info[\"object\"][\"sha\"]\n",
    "\n",
    "    # Create a new tree:\n",
    "    \n",
    "    tree = []\n",
    "    for f in all_files:\n",
    "        tree.append({\n",
    "            \"path\": f[\"sql_path\"],\n",
    "            \"mode\": \"100644\",\n",
    "            \"type\": \"blob\",\n",
    "            \"content\": f[\"sql_content\"],\n",
    "        })\n",
    "        tree.append({\n",
    "            \"path\": f[\"yaml_path\"],\n",
    "            \"mode\": \"100644\",\n",
    "            \"type\": \"blob\",\n",
    "            \"content\": f[\"yaml_content\"],\n",
    "        })\n",
    "\n",
    "    tree_resp = requests.post(\n",
    "        f\"{api_base}/git/trees\",\n",
    "        headers=headers,\n",
    "        json={\"base_tree\": base_sha, \"tree\": tree},\n",
    "    ).json()\n",
    "    new_tree_sha = tree_resp[\"sha\"]\n",
    "\n",
    "    # Create commit:\n",
    "    \n",
    "    commit_message = f\"Auto-generated standardized dbt models for {source_schema}\"\n",
    "    commit_resp = requests.post(\n",
    "        f\"{api_base}/git/commits\",\n",
    "        headers=headers,\n",
    "        json={\"message\": commit_message, \"tree\": new_tree_sha, \"parents\": [base_sha]},\n",
    "    ).json()\n",
    "    new_commit_sha = commit_resp[\"sha\"]\n",
    "\n",
    "    # Update branch reference\n",
    "    requests.patch(\n",
    "        f\"{api_base}/git/refs/heads/{branch}\",\n",
    "        headers=headers,\n",
    "        json={\"sha\": new_commit_sha},\n",
    "    )\n",
    "\n",
    "    print(f\" Successfully pushed {len(all_files)*2} files to {github_repo}@{branch}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb7076b7-649c-4ca9-b427-a736e81be975",
   "metadata": {
    "language": "python",
    "name": "cell31"
   },
   "outputs": [],
   "source": [
    "branch=\"main\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6eb66aa-ec5b-41e6-9721-f0fcd1bdb2da",
   "metadata": {
    "codeCollapsed": false,
    "language": "python",
    "name": "cell30"
   },
   "outputs": [],
   "source": [
    "create_all_standardized_views_dbt(\n",
    "    session=session,\n",
    "    source_db=TARGET_DB,\n",
    "    source_schema=SRC_SCHEMA,\n",
    "    target_schema=TARGET_SCHEMA,\n",
    "    model=MODEL,                \n",
    "    github_repo=GITHUB_REPO,\n",
    "    github_token=github_token,\n",
    "    branch=branch\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20de5b4d-bce6-4663-9ee4-36ddc79a0e23",
   "metadata": {
    "language": "python",
    "name": "cell14"
   },
   "outputs": [],
   "source": [
    "# def create_standardized_view_dbt(\n",
    "#     session,\n",
    "#     source_db: str,\n",
    "#     source_schema: str,\n",
    "#     source_table: str,\n",
    "#     target_schema: str,\n",
    "#     model: str,\n",
    "#     dbt_project_path: str\n",
    "# ):\n",
    "#     \"\"\"\n",
    "#     Generates standardized SQL + metadata for dbt.\n",
    "#     Writes the model (.sql) and schema (.yml) inside models/generated/.\n",
    "    \n",
    "#     \"\"\"\n",
    "\n",
    "#     # Skip tables not following naming convention:\n",
    "    \n",
    "#     if not source_table.startswith(\"SRC_\"):\n",
    "#         print(f\"Skipping {source_table}, must start with 'SRC_'.\")\n",
    "#         return None, None\n",
    "\n",
    "#     # Prepare names:\n",
    "    \n",
    "#     src_table_fq = f\"{source_db}.{source_schema}.{source_table}\"\n",
    "#     view_name = f\"V_STG_{source_table[len('SRC_'):] }_L1\"\n",
    "\n",
    "#     # Fetch schema info from Snowflake:\n",
    "    \n",
    "#     cols_df = session.sql(f\"\"\"\n",
    "#         SELECT COLUMN_NAME, DATA_TYPE, COMMENT\n",
    "#         FROM {source_db}.INFORMATION_SCHEMA.COLUMNS\n",
    "#         WHERE TABLE_SCHEMA = '{source_schema}'\n",
    "#           AND TABLE_NAME = '{source_table}'\n",
    "#         ORDER BY ORDINAL_POSITION\n",
    "#     \"\"\").to_pandas()\n",
    "\n",
    "#     schema_text = [\n",
    "#         {\"name\": r.COLUMN_NAME, \"type\": r.DATA_TYPE, \"description\": r.COMMENT or \"\"}\n",
    "#         for _, r in cols_df.iterrows()\n",
    "#     ]\n",
    "\n",
    "#     # Generate SQL + metadata via your LLM agent:\n",
    "    \n",
    "#     sql, metadata_json = call_l1_agent_snowflake(session, src_table_fq, schema_text, target_schema, model)\n",
    "#     metadata = json.loads(metadata_json)\n",
    "\n",
    "#     # Validate SQL:\n",
    "    \n",
    "#     validate_sql_snowflake(session, sql)\n",
    "\n",
    "#     # Ensure target directories:\n",
    "    \n",
    "#     models_dir = os.path.join(dbt_project_path, \"models\", \"generated\")\n",
    "#     os.makedirs(models_dir, exist_ok=True)\n",
    "\n",
    "#     # Write SQL model:\n",
    "    \n",
    "#     sql_path = os.path.join(models_dir, f\"{view_name}.sql\")\n",
    "#     with open(sql_path, \"w\") as f:\n",
    "#         f.write(textwrap.dedent(f\"\"\"\\\n",
    "#         -- Auto-generated by L1 Agent\n",
    "#         -- Source: {src_table_fq}\n",
    "#         -- Target Schema: {target_schema}\n",
    "#         -- Generated: {datetime.utcnow().isoformat()}\n",
    "\n",
    "#         {{% set target_schema = '{target_schema}' %}}\n",
    "\n",
    "#         {sql}\n",
    "#         \"\"\"))\n",
    "#     print(f\" SQL model written: {sql_path}\")\n",
    "\n",
    "#     #  Write metadata YAML :\n",
    "    \n",
    "#     yaml_path = os.path.join(models_dir, f\"{view_name}.yml\")\n",
    "#     column_entries = \"\\n\".join(\n",
    "#         [f\"      - name: {col['name']}\\n        description: \\\"{col.get('description', '')}\\\"\"\n",
    "#          for col in metadata.get(\"columns\", [])]\n",
    "#     )\n",
    "\n",
    "#     yaml_content = textwrap.dedent(f\"\"\"\\\n",
    "#     version: 2\n",
    "\n",
    "#     models:\n",
    "#       - name: {view_name}\n",
    "#         description: \"{metadata.get('table_description', '')}\"\n",
    "#         columns:\n",
    "#     {column_entries}\n",
    "#     \"\"\")\n",
    "#     with open(yaml_path, \"w\") as f:\n",
    "#         f.write(yaml_content)\n",
    "#     print(f\" Metadata YAML written: {yaml_path}\")\n",
    "\n",
    "#     return sql_path, yaml_path\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f63981a9-676c-48a0-889f-70b7413eaa9a",
   "metadata": {
    "collapsed": false,
    "id": "bnRZxaeQ6NJj",
    "jupyter": {
     "outputs_hidden": false
    },
    "name": "cell34"
   },
   "source": [
    "#### Process all the L0 tables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67ba473a-bd3a-431e-8d28-8713e6b8647a",
   "metadata": {
    "codeCollapsed": false,
    "executionInfo": {
     "elapsed": 709,
     "status": "ok",
     "timestamp": 1759937282348,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": -330
    },
    "id": "9cu-2yEF2_WV",
    "language": "python",
    "name": "cell35"
   },
   "outputs": [],
   "source": [
    "# def create_all_standardized_views_dbt(\n",
    "#     session,\n",
    "#     source_db: str,\n",
    "#     source_schema: str,\n",
    "#     target_schema: str,\n",
    "#     model: str,\n",
    "#     git_repo_url: str,\n",
    "#     branch: str = \"main\",\n",
    "#     local_repo_path: str = \"/tmp/appRoot/dbt_project\",\n",
    "#     github_token: str = None,\n",
    "#     auto_push: bool = True\n",
    "# ):\n",
    "#     \"\"\"\n",
    "#     Loops over all SRC_ tables in a Snowflake schema.\n",
    "#     Generates dbt models + YAML metadata for each.\n",
    "#     Clones/pulls repo once, pushes all changes at the end.\n",
    "#     \"\"\"\n",
    "\n",
    "#     # Clone or pull repo once :\n",
    "    \n",
    "#     repo_url = git_repo_url\n",
    "#     if github_token and repo_url.startswith(\"https://\"):\n",
    "#         repo_url = repo_url.replace(\"https://\", f\"https://{github_token}@\")\n",
    "\n",
    "#     if not os.path.exists(local_repo_path):\n",
    "#         print(f\"Cloning repo {git_repo_url}...\")\n",
    "#         subprocess.run([\"git\", \"clone\", \"-b\", branch, repo_url, local_repo_path], check=True)\n",
    "#     else:\n",
    "#         print(f\"Pulling latest changes from {branch}...\")\n",
    "#         subprocess.run([\"git\", \"-C\", local_repo_path, \"fetch\", \"origin\"], check=True)\n",
    "#         subprocess.run([\"git\", \"-C\", local_repo_path, \"checkout\", branch], check=True)\n",
    "#         subprocess.run([\"git\", \"-C\", local_repo_path, \"pull\"], check=True)\n",
    "\n",
    "#     #  Fetch all table names :\n",
    "    \n",
    "#     tables_df = session.sql(f\"\"\"\n",
    "#         SELECT TABLE_NAME\n",
    "#         FROM {source_db}.INFORMATION_SCHEMA.TABLES\n",
    "#         WHERE TABLE_SCHEMA = '{source_schema}'\n",
    "#           AND TABLE_TYPE = 'BASE TABLE'\n",
    "#     \"\"\").to_pandas()\n",
    "\n",
    "#     if tables_df.empty:\n",
    "#         print(f\"No tables found in {source_db}.{source_schema}\")\n",
    "#         return\n",
    "\n",
    "#     # Generate dbt models:\n",
    "    \n",
    "#     created_models = []\n",
    "#     for _, row in tables_df.iterrows():\n",
    "#         source_table = row[\"TABLE_NAME\"]\n",
    "\n",
    "#         if not source_table.startswith(\"SRC_\"):\n",
    "#             print(f\"Skipping {source_table} (not an SRC_ table)\")\n",
    "#             continue\n",
    "\n",
    "#         try:\n",
    "#             print(f\"\\n=== Processing table: {source_table} ===\")\n",
    "#             sql_path, yaml_path = create_standardized_view_dbt(\n",
    "#                 session=session,\n",
    "#                 source_db=source_db,\n",
    "#                 source_schema=source_schema,\n",
    "#                 source_table=source_table,\n",
    "#                 target_schema=target_schema,\n",
    "#                 model=model,\n",
    "#                 dbt_project_path=local_repo_path\n",
    "#             )\n",
    "#             if sql_path:\n",
    "#                 created_models.append((source_table, sql_path, yaml_path))\n",
    "#         except Exception as e:\n",
    "#             print(f\" Failed for {source_table}: {e}\")\n",
    "\n",
    "#     #  Commit & push once :\n",
    "    \n",
    "#     if auto_push and created_models:\n",
    "#         subprocess.run([\"git\", \"-C\", local_repo_path, \"add\", \".\"], check=True)\n",
    "#         subprocess.run(\n",
    "#             [\"git\", \"-C\", local_repo_path, \"commit\", \"-m\", f\"Add {len(created_models)} new dbt models\"],\n",
    "#             check=False  # ignore \"nothing to commit\"\n",
    "#         )\n",
    "#         subprocess.run([\"git\", \"-C\", local_repo_path, \"push\", \"origin\", branch], check=True)\n",
    "#         print(f\" {len(created_models)} models pushed to {branch}\")\n",
    "\n",
    "#     #  Summary :\n",
    "    \n",
    "#     print(\"\\n Summary of generated models:\")\n",
    "#     for tbl, sql_p, yml_p in created_models:\n",
    "#         print(f\"  - {tbl}:\")\n",
    "#         print(f\"      SQL : {sql_p}\")\n",
    "#         print(f\"      YAML: {yml_p}\")\n",
    "\n",
    "#     print(\"\\n All eligible tables processed successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c971a618-d029-4880-bbcb-68288c9da037",
   "metadata": {
    "collapsed": false,
    "id": "dFR2ywwy8qL2",
    "jupyter": {
     "outputs_hidden": false
    },
    "name": "cell36"
   },
   "source": [
    "#### Execute L1 Workflow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5d25ab1-6e20-4198-b180-15d2b5c52bf0",
   "metadata": {
    "language": "python",
    "name": "cell15"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7fbc279-1c03-40d9-a804-6bda2ee9497c",
   "metadata": {
    "codeCollapsed": false,
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 88765,
     "status": "ok",
     "timestamp": 1759938635988,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": -330
    },
    "id": "wwl9gww5gtzH",
    "language": "python",
    "name": "cell37",
    "outputId": "87ace9cf-8738-4297-b40a-7bdbc2f16ffa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset dataplatr-sandbox.L1_sageintact already exists. Doing nothing.\n",
      "Folder definitions/L0 may already exist: 409 Directory already exists at definitions/L0\n",
      "Wrote source declaration to definitions/L0/sageintact_SRC_SI_GLACCOUNT.sqlx\n",
      "{'name': 'V_STG_SI_GLACCOUNT_L1', 'table_description': \"The SRC_SI_GLACCOUNT table stores the master data for all general ledger (GL) accounts, defining the organization's chart of accounts. Each record represents a unique GL account, detailing its characteristics, classification, and behavior within the financial system. It serves as a foundational reference table, linked by transactional tables such as SRC_SI_GLENTRY and SRC_SI_GLBATCH to associate financial transactions with specific accounts. The table includes key attributes such as the unique account number, descriptive title, account type (e.g., balance sheet, income statement), normal balance (debit or credit), and how it's handled during period-end closing. It also captures various categorization details (e.g., category, category key) and operational requirements, such as whether a department, location, business unit, project, customer, vendor, employee, item, class, or warehouse is required for postings. Furthermore, it includes flags for taxability, subledger control, and GL matching, along with alternative account mappings and specific asset-related GL account references. This table is crucial for maintaining the integrity and structure of the financial system, enabling accurate financial reporting, budgeting, and analysis by ensuring all financial activities are categorized and tracked consistently. It also captures metadata related to the account's status, creation, modification, and deletion, including timestamps, user IDs, and organizational context (mega entity). The table supports financial compliance and auditing requirements by providing a comprehensive definition of each GL account. It also includes metadata related to data loading and change tracking from the source system.\\n\\nExamples of use cases:\\n\\n1. Defining and managing the organization's chart of accounts.\\n2. Providing the foundational structure for all financial transactions recorded in SRC_SI_GLENTRY and SRC_SI_GLBATCH.\\n3. Controlling the dimensional requirements for GL postings (e.g., requiring a specific department or project for certain accounts).\\n4. Supporting financial reporting by classifying accounts into types (e.g., assets, liabilities, revenue, expense).\\n5. Auditing financial data by providing detailed definitions and characteristics of each GL account.\\n6. Managing the lifecycle of GL accounts, including activation, deactivation, and changes to their properties.\", 'columns': [{'name': 'RCIP_ASSET_GL_ACCOUNT', 'description': 'Reference to a GL account specifically for capitalized assets, used in certain accounting processes.'}, {'name': 'RECORD_NO', 'description': 'Unique record identifier for the GL account within the system'}, {'name': 'ACCOUNT_NO', 'description': 'The unique alphanumeric code identifying the general ledger account'}, {'name': 'TITLE', 'description': 'Descriptive name or title of the GL account'}, {'name': 'ACCOUNT_TYPE', 'description': 'Type of GL account (e.g., balance sheet, income statement, etc.)'}, {'name': 'NORMAL_BALANCE', 'description': 'Indicates whether the account typically carries a debit or credit balance'}, {'name': 'CLOSING_TYPE', 'description': 'Indicates how the account is handled during the period-end closing process (e.g., non-closing account, closing to another account).'}, {'name': 'REQUIRE_DEPT', 'description': 'Indicates if a Department is required for postings'}, {'name': 'REQUIRE_LOC', 'description': 'Indicates if a Location is required for postings'}, {'name': 'TAXABLE', 'description': 'Flag indicating whether the account is subject to tax.'}, {'name': 'CATEGORY', 'description': 'Category grouping for the GL account'}, {'name': 'CATEGORY_KEY', 'description': 'Key/identifier for the account category'}, {'name': 'TAX_CODE', 'description': 'Tax code associated with the GL account'}, {'name': 'MRC_CODE', 'description': 'MRC (Multi-Reporting Code) or similar classification'}, {'name': 'CLOSE_TO_ACCT_KEY', 'description': 'Key of the account to which this account closes during period-end'}, {'name': 'ALTERNATIVE_ACCOUNT', 'description': 'Alternative GL account mapping (if applicable)'}, {'name': 'SUBLEDGER_CONTROL_ON', 'description': 'Indicates if subledger control is enabled for this account'}, {'name': 'WIP_SETUP_ACCT_TYPE', 'description': 'Work-In-Progress setup account type'}, {'name': 'ENABLE_GL_MATCHING', 'description': 'Flag indicating if GL matching is enabled'}, {'name': 'LETTRAGE_SEQUENCE_ID', 'description': 'Identifier for lettrage (matching/reconciliation sequence)'}, {'name': 'STATUS', 'description': 'Current status of the account (e.g., Active, Inactive)'}, {'name': 'LAST_MODIFIED_DATE', 'description': 'Timestamp of the last modification'}, {'name': 'CREATED_DATE', 'description': 'Timestamp of when the record was created'}, {'name': 'CREATED_BY', 'description': 'User ID of who created the record'}, {'name': 'MODIFIED_BY', 'description': 'User ID of who last modified the record'}, {'name': 'MEGA_ENTITY_KEY', 'description': 'Identifier key for the mega entity (organization)'}, {'name': 'MEGA_ENTITY_ID', 'description': 'Code for the mega entity (organization)'}, {'name': 'MEGA_ENTITY_NAME', 'description': 'Name of the mega entity (organization)'}, {'name': 'REQUIRE_GL_DIM_BUSINESS_UNIT', 'description': 'Indicates if Business Unit is required'}, {'name': 'REQUIRE_PROJECT', 'description': 'Indicates if Project is required'}, {'name': 'REQUIRE_CUSTOMER', 'description': 'Indicates if Customer is required'}, {'name': 'REQUIRE_VENDOR', 'description': 'Indicates if Vendor is required'}, {'name': 'REQUIRE_EMPLOYEE', 'description': 'Indicates if Employee is required'}, {'name': 'REQUIRE_ITEM', 'description': 'Indicates if Item/Product is required'}, {'name': 'REQUIRE_CLASS', 'description': 'Indicates if Class is required'}, {'name': 'REQUIRE_WAREHOUSE', 'description': 'Indicates if Warehouse is required'}, {'name': 'DDS_READ_TIME', 'description': 'Timestamp when record was last read from the data source'}, {'name': 'LOAD_DATE', 'description': 'Date when record was loaded into the system'}, {'name': 'DELETED_BY', 'description': 'User ID who deleted the record'}, {'name': 'WHEN_DELETED', 'description': 'Timestamp of when the record was deleted'}, {'name': 'DDS_CHANGE_TYPE', 'description': 'Type of change captured (Insert, Update, Delete) in data pipeline'}, {'name': 'IS_DELETED', 'description': 'Flag indicating if the record is marked as deleted'}]}\n",
      "Generated SQL for SRC_SI_GLACCOUNT:\n",
      "SELECT RCIPASSETGLACCOUNT AS RCIP_ASSET_GL_ACCOUNT,\n",
      "       RECORDNO AS RECORD_NO,\n",
      "       ACCOUNTNO AS ACCOUNT_NO,\n",
      "       TITLE AS TITLE,\n",
      "       ACCOUNTTYPE AS ACCOUNT_TYPE,\n",
      "       NORMALBALANCE AS NORMAL_BALANCE,\n",
      "       CLOSINGTYPE AS CLOSING_TYPE,\n",
      "       REQUIREDEPT AS REQUIRE_DEPT,\n",
      "       REQUIRELOC AS REQUIRE_LOC,\n",
      "       TAXABLE AS TAXABLE,\n",
      "       CATEGORY AS CATEGORY,\n",
      "       CATEGORYKEY AS CATEGORY_KEY,\n",
      "       TAXCODE AS TAX_CODE,\n",
      "       MRCCODE AS MRC_CODE,\n",
      "       CLOSETOACCTKEY AS CLOSE_TO_ACCT_KEY,\n",
      "       ALTERNATIVEACCOUNT AS ALTERNATIVE_ACCOUNT,\n",
      "       SUBLEDGERCONTROLON AS SUBLEDGER_CONTROL_ON,\n",
      "       WIPSETUPACCTTYPE AS WIP_SETUP_ACCT_TYPE,\n",
      "       ENABLE_GLMATCHING AS ENABLE_GL_MATCHING,\n",
      "       LETTRAGESEQUENCEID AS LETTRAGE_SEQUENCE_ID,\n",
      "       STATUS AS STATUS,\n",
      "       WHENMODIFIED AS LAST_MODIFIED_DATE,\n",
      "       WHENCREATED AS CREATED_DATE,\n",
      "       CREATEDBY AS CREATED_BY,\n",
      "       MODIFIEDBY AS MODIFIED_BY,\n",
      "       MEGAENTITYKEY AS MEGA_ENTITY_KEY,\n",
      "       MEGAENTITYID AS MEGA_ENTITY_ID,\n",
      "       MEGAENTITYNAME AS MEGA_ENTITY_NAME,\n",
      "       REQUIREGLDIMBUSINESS_UNIT AS REQUIRE_GL_DIM_BUSINESS_UNIT,\n",
      "       REQUIREPROJECT AS REQUIRE_PROJECT,\n",
      "       REQUIRECUSTOMER AS REQUIRE_CUSTOMER,\n",
      "       REQUIREVENDOR AS REQUIRE_VENDOR,\n",
      "       REQUIREEMPLOYEE AS REQUIRE_EMPLOYEE,\n",
      "       REQUIREITEM AS REQUIRE_ITEM,\n",
      "       REQUIRECLASS AS REQUIRE_CLASS,\n",
      "       REQUIREWAREHOUSE AS REQUIRE_WAREHOUSE,\n",
      "       DDSREADTIME AS DDS_READ_TIME,\n",
      "       LOAD_DATE AS LOAD_DATE,\n",
      "       DELETEDBY AS DELETED_BY,\n",
      "       WHENDELETED AS WHEN_DELETED,\n",
      "       DDSCHANGETYPE AS DDS_CHANGE_TYPE,\n",
      "       ISDELETED AS IS_DELETED\n",
      "FROM `dataplatr-sandbox.sageintact.SRC_SI_GLACCOUNT`\n",
      " SQL is valid.\n",
      "Folder definitions/L1 may already exist: 409 Directory already exists at definitions/L1\n",
      " Wrote definitions/L1/V_STG_SI_GLACCOUNT_L1_metadata.json into Dataform workspace\n",
      "Folder definitions/L1 may already exist: 409 Directory already exists at definitions/L1\n",
      " Wrote definitions/L1/V_STG_SI_GLACCOUNT_L1.sqlx into Dataform workspace\n",
      "Folder definitions/L0 may already exist: 409 Directory already exists at definitions/L0\n",
      "Wrote source declaration to definitions/L0/sageintact_SRC_SI_GLBATCH.sqlx\n",
      "{'name': 'V_STG_SI_GLBATCH_L1', 'table_description': \"The SRC_SI_GLBATCH table stores metadata for batches of general ledger (GL) entries, providing a comprehensive record of grouped financial transactions. Each record represents a unique batch, containing multiple GL entries (detailed in SRC_SI_GLENTRY) posted together.  It acts as a control mechanism for GL postings, linking to other tables such as SRC_SI_GLACCOUNT (for the accounts involved), SRC_SI_GLENTRY (for individual entries within the batch), and SRC_SI_GLJOURNAL (for the journal where the batch was posted). The table includes key attributes such as the batch date, source module, user information (creator and modifier), status (e.g., posted, reversed), and various reference keys linking to related transactions and supporting documents (e.g., fixed assets, purchase orders, invoices).  It also captures metadata related to the batch's creation, modification, reversal, and deletion, including timestamps and user IDs.  The table supports various batch types and includes flags indicating reversal status and links to parent batches (for consolidated batches).  It also contains key identifiers for tracking and reconciliation, including batch numbers, titles, and external reference numbers. This table is crucial for managing GL transaction posting, auditing batch activity, and providing a comprehensive view of the financial data flow within the system.  It also supports financial reporting and compliance requirements by providing detailed batch metadata, facilitating reconciliation and analysis of financial data at the batch level.  The table includes timestamps for creation and modification, user IDs, and flags indicating deletion status. It also includes metadata related to data loading and change tracking from the source system.\\n\\nExamples of use cases:\\n\\n1. Tracking groups of financial transactions posted together.\\n2. Auditing GL transactions by linking batches to accounts, entries, and journals.\\n3. Supporting financial reporting and analysis by providing summarized transactional data at the batch level.\\n4. Reconciling GL transactions by tracing back to source batches.\\n5. Analyzing batch patterns and identifying potential anomalies.\\n6. Managing the lifecycle of batches (e.g., creation, posting, reversal, deletion).\\n7. Identifying batches associated with specific modules or processes (e.g., fixed assets, accounts payable).\", 'columns': [{'name': 'R_ASSET', 'description': 'Internal reference key if the batch originated from a fixed asset transaction.'}, {'name': 'R_DEPRECIATION_SCHEDULE', 'description': 'Reference key to a depreciation schedule when the batch is generated from Fixed Assets.'}, {'name': 'R_ASSET_ADJUSTMENT', 'description': 'Reference key for any fixed asset adjustments associated with this batch.'}, {'name': 'R_ASSET_CLASS', 'description': 'Asset class linked to the batch (when coming from the Fixed Assets module).'}, {'name': 'R_ASSET_OUT_OF_SERVICE', 'description': 'Reference key for fixed assets taken out of service, if this batch reflects such transactions.'}, {'name': 'BATCH_DATE', 'description': 'The effective date of the journal batch; used for posting into GL.'}, {'name': 'MODULE', 'description': 'The source module that created the batch (e.g., GL, AP, AR, FA).'}, {'name': 'USER_KEY', 'description': 'Internal user key of the person who created or posted the batch.'}, {'name': 'MODIFIED', 'description': 'Date/time when the batch was last modified.'}, {'name': 'REFERENCE_NO', 'description': 'External or internal reference number associated with the batch (e.g., import ID, source transaction reference).'}, {'name': 'REVERSED', 'description': 'Flag or code indicating whether this batch was reversed.'}, {'name': 'REVERSED_KEY', 'description': 'Internal reference to the batch that reversed this one.'}, {'name': 'TEMPLATE_KEY', 'description': 'Reference to a recurring journal template if the batch originated from one.'}, {'name': 'PR_BATCH_KEY', 'description': 'Link to the parent batch key if part of a consolidated batch.'}, {'name': 'SCH_OP_KEY', 'description': 'Schedule or operational key linking the batch to scheduled postings.'}, {'name': 'REVERSED_FROM', 'description': 'Date the batch was reversed from (if reversal was applied).'}, {'name': 'SUP_DOC_ID', 'description': 'Supporting document ID (user-defined), often used to attach external references.'}, {'name': 'STATE', 'description': 'Status of the batch (e.g., posted, in-progress, reversed).'}, {'name': 'JOURNAL_SEQ_NO', 'description': 'Journal sequence number within the batch for audit tracking.'}, {'name': 'TAX_IMPLICATIONS', 'description': 'Tax handling details if this batch involves tax postings (e.g., VAT).'}, {'name': 'VAT_VENDOR_ID', 'description': 'Vendor ID used for VAT-related postings.'}, {'name': 'VAT_CUSTOMER_ID', 'description': 'Customer ID used for VAT-related postings.'}, {'name': 'VAT_CONTACT_ID', 'description': 'Contact ID associated with VAT processing.'}, {'name': 'TAX_SOLUTION_ID', 'description': 'Identifier for the tax solution applied to this batch (if integrated with tax software).'}, {'name': 'GL_ACCT_ALLOCATION_RUN_KEY', 'description': 'Allocation run key if this batch is generated from GL account allocation.'}, {'name': 'SUP_DOC_KEY', 'description': 'Internal key reference for the supporting document (links SUPDOCID).'}, {'name': 'WIP_RELIEF_RUN_KEY', 'description': 'Key reference when batch is tied to Work-In-Progress relief runs.'}, {'name': 'LAST_MODIFIED_DATE', 'description': 'Timestamp when the batch was last modified.'}, {'name': 'CREATED_DATE', 'description': 'Timestamp when the batch was created.'}, {'name': 'CREATED_BY', 'description': 'Internal user ID who created the batch.'}, {'name': 'MODIFIED_BY', 'description': 'Internal user ID who modified the batch.'}, {'name': 'MEGA_ENTITY_KEY', 'description': 'Internal key for the top-level entity (in multi-entity setup).'}, {'name': 'MEGA_ENTITY_ID', 'description': 'Entity ID for the top-level entity.'}, {'name': 'MEGA_ENTITY_NAME', 'description': 'Entity name for the top-level entity.'}, {'name': 'DDS_READ_TIME', 'description': 'Timestamp when the record was read/exported by DDS.'}, {'name': 'LOAD_DATE', 'description': 'Timestamp when loaded into the data warehouse or ETL layer.'}, {'name': 'DELETED_BY', 'description': 'User ID that deleted the batch (if deleted).'}, {'name': 'WHEN_DELETED', 'description': 'Timestamp when the batch was deleted.'}, {'name': 'DDS_CHANGE_TYPE', 'description': 'DDS change tracking value (‘I’=Insert, ‘U’=Update, ‘D’=Delete).'}, {'name': 'IS_DELETED', 'description': 'Boolean flag indicating soft delete status (TRUE/FALSE).'}, {'name': 'RECORD_NO', 'description': 'Primary key / record number of the batch in Intacct.'}, {'name': 'BATCH_NO', 'description': 'Unique identifier for the general ledger batch. Each batch of GL entries is assigned a distinct BATCHNO for tracking and referencing purposes.'}, {'name': 'BATCH_TITLE', 'description': \"A descriptive title for the batch, often including source, date, and time information. Examples include 'Receiver: 03/18/2025 14:31:36:1540 Purchase Batch' or 'Bills: 2025/01/02 Batch Summary Entry'. Provides a human-readable summary of the batch's contents.\"}, {'name': 'JOURNAL', 'description': 'Journal (by symbol) where this batch was posted (ties to GLJOURNAL).'}]}\n",
      "Generated SQL for SRC_SI_GLBATCH:\n",
      "SELECT RASSET AS R_ASSET,\n",
      "       RDEPRECIATION_SCHEDULE AS R_DEPRECIATION_SCHEDULE,\n",
      "       RASSET_ADJUSTMENT AS R_ASSET_ADJUSTMENT,\n",
      "       RASSET_CLASS AS R_ASSET_CLASS,\n",
      "       RASSETOUTOFSERVICE AS R_ASSET_OUT_OF_SERVICE,\n",
      "       BATCH_DATE,\n",
      "       MODULE,\n",
      "       USERKEY AS USER_KEY,\n",
      "       MODIFIED,\n",
      "       REFERENCENO AS REFERENCE_NO,\n",
      "       REVERSED,\n",
      "       REVERSEDKEY AS REVERSED_KEY,\n",
      "       TEMPLATEKEY AS TEMPLATE_KEY,\n",
      "       PRBATCHKEY AS PR_BATCH_KEY,\n",
      "       SCHOPKEY AS SCH_OP_KEY,\n",
      "       REVERSEDFROM AS REVERSED_FROM,\n",
      "       SUPDOCID AS SUP_DOC_ID,\n",
      "       STATE,\n",
      "       JOURNALSEQNO AS JOURNAL_SEQ_NO,\n",
      "       TAXIMPLICATIONS AS TAX_IMPLICATIONS,\n",
      "       VATVENDORID AS VAT_VENDOR_ID,\n",
      "       VATCUSTOMERID AS VAT_CUSTOMER_ID,\n",
      "       VATCONTACTID AS VAT_CONTACT_ID,\n",
      "       TAXSOLUTIONID AS TAX_SOLUTION_ID,\n",
      "       GLACCTALLOCATIONRUNKEY AS GL_ACCT_ALLOCATION_RUN_KEY,\n",
      "       SUPDOCKEY AS SUP_DOC_KEY,\n",
      "       WIPRELIEFRUNKEY AS WIP_RELIEF_RUN_KEY,\n",
      "       WHENMODIFIED AS LAST_MODIFIED_DATE,\n",
      "       WHENCREATED AS CREATED_DATE,\n",
      "       CREATEDBY AS CREATED_BY,\n",
      "       MODIFIEDBY AS MODIFIED_BY,\n",
      "       MEGAENTITYKEY AS MEGA_ENTITY_KEY,\n",
      "       MEGAENTITYID AS MEGA_ENTITY_ID,\n",
      "       MEGAENTITYNAME AS MEGA_ENTITY_NAME,\n",
      "       DDSREADTIME AS DDS_READ_TIME,\n",
      "       LOAD_DATE,\n",
      "       DELETEDBY AS DELETED_BY,\n",
      "       WHENDELETED AS WHEN_DELETED,\n",
      "       DDSCHANGETYPE AS DDS_CHANGE_TYPE,\n",
      "       ISDELETED AS IS_DELETED,\n",
      "       RECORDNO AS RECORD_NO,\n",
      "       BATCHNO AS BATCH_NO,\n",
      "       BATCH_TITLE,\n",
      "       JOURNAL\n",
      "FROM `dataplatr-sandbox.sageintact.SRC_SI_GLBATCH`\n",
      " SQL is valid.\n",
      "Folder definitions/L1 may already exist: 409 Directory already exists at definitions/L1\n",
      " Wrote definitions/L1/V_STG_SI_GLBATCH_L1_metadata.json into Dataform workspace\n",
      "Folder definitions/L1 may already exist: 409 Directory already exists at definitions/L1\n",
      " Wrote definitions/L1/V_STG_SI_GLBATCH_L1.sqlx into Dataform workspace\n",
      "Folder definitions/L0 may already exist: 409 Directory already exists at definitions/L0\n",
      "Wrote source declaration to definitions/L0/sageintact_SRC_SI_GLENTRY.sqlx\n",
      "{'name': 'V_STG_SI_GLENTRY_L1', 'table_description': \"The SRC_SI_GLENTRY table stores individual general ledger (GL) entries, representing the fundamental transactional records within the financial system. Each record details a single financial transaction, providing a comprehensive audit trail of all GL activity.  It acts as a core component of the GL system, linking to other tables such as SRC_SI_GLACCOUNT (for the accounts involved), SRC_SI_GLBATCH (for the batch to which the entry belongs), and SRC_SI_GLJOURNAL (for the journal where the entry was recorded). The table includes key transactional attributes such as entry date, document number, description, account number, amount (in both transaction and base currency), and various dimensions (business unit, location, department, etc.) providing context and granularity to each entry.  It also captures metadata related to the transaction's creation, modification, and reconciliation, including timestamps, user IDs, and status flags (cleared, adjusted, deleted).  The table supports various transaction types (debit, credit, adjustment), and includes flags indicating billing status and whether the entry is statistical or monetary.  It also contains key identifiers for tracking and reconciliation, including record numbers, batch numbers, and links to related master data tables (e.g., vendors, customers, employees). This table is crucial for maintaining a detailed history of all financial transactions, supporting financial reporting, auditing, and compliance requirements.  It also facilitates reconciliation and analysis of financial data at a granular level.  The table includes timestamps for creation and modification, user IDs, and flags indicating deletion status.  It also includes metadata related to data loading and change tracking from the source system.\\n\\nExamples of use cases:\\n\\n1. Tracking individual financial transactions.\\n2. Auditing GL transactions by linking entries to accounts, batches, and journals.\\n3. Supporting financial reporting and analysis by providing detailed transactional data.\\n4. Reconciling GL transactions by tracing back to source entries.\\n5. Analyzing transaction patterns and identifying potential anomalies.\\n6. Managing the lifecycle of entries (e.g., creation, posting, reversal, deletion).\\n7. Identifying entries associated with specific dimensions (e.g., department, location, project).\\n8. Maintaining a comprehensive and accurate record of all financial activity.\", 'columns': [{'name': 'GL_DIM_BUSINESS_UNIT', 'description': 'Business Unit dimension used in the GL entry'}, {'name': 'RECORD_NO', 'description': 'Unique identifier for the GL entry record'}, {'name': 'ENTRY_DATE', 'description': 'Date of the general ledger entry'}, {'name': 'DOCUMENT', 'description': 'Document number or identifier associated with the general ledger entry'}, {'name': 'DESCRIPTION', 'description': 'Description of the general ledger entry, providing context for the transaction'}, {'name': 'ACCOUNT_NO', 'description': 'Account number involved in the general ledger entry'}, {'name': 'AMOUNT', 'description': 'The transactional amount of the general ledger entry. Represents the debit or credit value of the financial transaction.'}, {'name': 'TRX_AMOUNT', 'description': 'Transaction amount in original transaction currency'}, {'name': 'BASE_CURR', 'description': 'Base currency code of the company'}, {'name': 'CURRENCY', 'description': 'Original transaction currency code for the general ledger entry'}, {'name': 'USER_NO', 'description': 'Identifier for the user associated with the entry'}, {'name': 'LOCATION', 'description': 'Location dimension associated with the entry'}, {'name': 'DEPARTMENT', 'description': 'Department dimension associated with the entry'}, {'name': 'LINE_NO', 'description': 'Line number within the journal or document'}, {'name': 'TR_TYPE', 'description': 'Transaction type code (e.g., debit, credit, adjustment)'}, {'name': 'ADJ', 'description': \"Indicates whether the general ledger entry is an adjustment. 'F' likely signifies a formal or finalized adjustment.\"}, {'name': 'CLEARED', 'description': \"Indicates whether the general ledger entry has been cleared. 'F' signifies that it is not yet cleared.\"}, {'name': 'TIME_PERIOD', 'description': 'Accounting period (month/year) applicable to the entry'}, {'name': 'EXCH_RATE_DATE', 'description': 'Date used to determine the exchange rate'}, {'name': 'EXCH_RATE_TYPE_ID', 'description': 'Exchange rate type (e.g., spot, average, corporate)'}, {'name': 'EXCHANGE_RATE', 'description': 'Exchange rate applied to convert transaction currency to base currency'}, {'name': 'RECON_DATE', 'description': 'Reconciliation date for the entry'}, {'name': 'CLR_DATE', 'description': 'Date the entry was cleared'}, {'name': 'BATCH_NO', 'description': 'Identifier for the batch to which this entry belongs'}, {'name': 'ACCOUNT_KEY', 'description': 'Key/ID representing the account dimension'}, {'name': 'DEPARTMENT_KEY', 'description': 'Key/ID representing the department dimension'}, {'name': 'LOCATION_KEY', 'description': 'Key/ID representing the location dimension'}, {'name': 'ALLOCATION_KEY', 'description': 'Key/ID for allocation rules if applicable'}, {'name': 'STATE', 'description': 'Current posting state of the entry (e.g., posted, unposted)'}, {'name': 'BILLABLE', 'description': 'Flag indicating if the entry is billable'}, {'name': 'BILLED', 'description': 'Flag indicating if the entry has been billed'}, {'name': 'PROJECT_ID', 'description': 'Project identifier linked to the entry'}, {'name': 'PROJECT_DIM_KEY', 'description': 'Key/ID for the project dimension'}, {'name': 'CUSTOMER_ID', 'description': 'Customer identifier linked to the entry'}, {'name': 'CUSTOMER_DIM_KEY', 'description': 'Key/ID for the customer dimension'}, {'name': 'VENDOR_ID', 'description': 'Vendor identifier linked to the entry'}, {'name': 'VENDOR_DIM_KEY', 'description': 'Key/ID for the vendor dimension'}, {'name': 'EMPLOYEE_ID', 'description': 'Employee identifier linked to the entry'}, {'name': 'EMPLOYEE_DIM_KEY', 'description': 'Key/ID for the employee dimension'}, {'name': 'ITEM_ID', 'description': 'Item/product identifier linked to the entry'}, {'name': 'ITEM_DIM_KEY', 'description': 'Key/ID for the item dimension'}, {'name': 'CLASS_ID', 'description': 'Class identifier linked to the entry'}, {'name': 'CLASS_DIM_KEY', 'description': 'Key/ID for the class dimension'}, {'name': 'WAREHOUSE_ID', 'description': 'Warehouse identifier linked to the entry'}, {'name': 'WAREHOUSE_DIM_KEY', 'description': 'Key/ID for the warehouse dimension'}, {'name': 'LAST_MODIFIED_DATE', 'description': 'Timestamp when the record was last modified'}, {'name': 'CREATED_DATE', 'description': 'Timestamp when the record was created'}, {'name': 'CREATED_BY', 'description': 'User ID of the creator of the entry'}, {'name': 'MODIFIED_BY', 'description': 'User ID of the last modifier of the entry'}, {'name': 'SYSTEM_SIZE', 'description': 'NA (system-specific metadata, unclear without context)'}, {'name': 'BATCH_TITLE', 'description': 'Title/description of the batch'}, {'name': 'STATISTICAL', 'description': 'Flag indicating if entry is statistical (non-monetary, e.g., headcount)'}, {'name': 'DDS_READ_TIME', 'description': 'Timestamp when record was read from the data source'}, {'name': 'LOAD_DATE', 'description': 'Date when record was loaded into the system'}, {'name': 'DELETED_BY', 'description': 'User ID who deleted the record'}, {'name': 'WHEN_DELETED', 'description': 'Timestamp when the record was deleted'}, {'name': 'DDS_CHANGE_TYPE', 'description': 'Type of change captured (Insert, Update, Delete) in data pipeline'}, {'name': 'IS_DELETED', 'description': 'Flag indicating if the record is marked as deleted'}]}\n",
      "Generated SQL for SRC_SI_GLENTRY:\n",
      "SELECT GLDIMBUSINESS_UNIT AS GL_DIM_BUSINESS_UNIT,\n",
      "       RECORDNO AS RECORD_NO,\n",
      "       ENTRY_DATE,\n",
      "       DOCUMENT,\n",
      "       DESCRIPTION,\n",
      "       ACCOUNTNO AS ACCOUNT_NO,\n",
      "       AMOUNT,\n",
      "       TRX_AMOUNT,\n",
      "       BASECURR AS BASE_CURR,\n",
      "       CURRENCY,\n",
      "       USERNO AS USER_NO,\n",
      "       LOCATION,\n",
      "       DEPARTMENT,\n",
      "       LINE_NO,\n",
      "       TR_TYPE,\n",
      "       ADJ,\n",
      "       CLEARED,\n",
      "       TIMEPERIOD AS TIME_PERIOD,\n",
      "       EXCH_RATE_DATE,\n",
      "       EXCH_RATE_TYPE_ID,\n",
      "       EXCHANGE_RATE,\n",
      "       RECON_DATE,\n",
      "       CLRDATE AS CLR_DATE,\n",
      "       BATCHNO AS BATCH_NO,\n",
      "       ACCOUNTKEY AS ACCOUNT_KEY,\n",
      "       DEPARTMENTKEY AS DEPARTMENT_KEY,\n",
      "       LOCATIONKEY AS LOCATION_KEY,\n",
      "       ALLOCATIONKEY AS ALLOCATION_KEY,\n",
      "       STATE,\n",
      "       BILLABLE,\n",
      "       BILLED,\n",
      "       PROJECTID AS PROJECT_ID,\n",
      "       PROJECTDIMKEY AS PROJECT_DIM_KEY,\n",
      "       CUSTOMERID AS CUSTOMER_ID,\n",
      "       CUSTOMERDIMKEY AS CUSTOMER_DIM_KEY,\n",
      "       VENDORID AS VENDOR_ID,\n",
      "       VENDORDIMKEY AS VENDOR_DIM_KEY,\n",
      "       EMPLOYEEID AS EMPLOYEE_ID,\n",
      "       EMPLOYEEDIMKEY AS EMPLOYEE_DIM_KEY,\n",
      "       ITEMID AS ITEM_ID,\n",
      "       ITEMDIMKEY AS ITEM_DIM_KEY,\n",
      "       CLASSID AS CLASS_ID,\n",
      "       CLASSDIMKEY AS CLASS_DIM_KEY,\n",
      "       WAREHOUSEID AS WAREHOUSE_ID,\n",
      "       WAREHOUSEDIMKEY AS WAREHOUSE_DIM_KEY,\n",
      "       WHENMODIFIED AS LAST_MODIFIED_DATE,\n",
      "       WHENCREATED AS CREATED_DATE,\n",
      "       CREATEDBY AS CREATED_BY,\n",
      "       MODIFIEDBY AS MODIFIED_BY,\n",
      "       SYSTEM_SIZE,\n",
      "       BATCHTITLE AS BATCH_TITLE,\n",
      "       STATISTICAL,\n",
      "       DDSREADTIME AS DDS_READ_TIME,\n",
      "       LOAD_DATE,\n",
      "       DELETEDBY AS DELETED_BY,\n",
      "       WHENDELETED AS WHEN_DELETED,\n",
      "       DDSCHANGETYPE AS DDS_CHANGE_TYPE,\n",
      "       ISDELETED AS IS_DELETED\n",
      "FROM `dataplatr-sandbox.sageintact.SRC_SI_GLENTRY`\n",
      " SQL is valid.\n",
      "Folder definitions/L1 may already exist: 409 Directory already exists at definitions/L1\n",
      " Wrote definitions/L1/V_STG_SI_GLENTRY_L1_metadata.json into Dataform workspace\n",
      "Folder definitions/L1 may already exist: 409 Directory already exists at definitions/L1\n",
      " Wrote definitions/L1/V_STG_SI_GLENTRY_L1.sqlx into Dataform workspace\n",
      "Folder definitions/L0 may already exist: 409 Directory already exists at definitions/L0\n",
      "Wrote source declaration to definitions/L0/sageintact_SRC_SI_GLJOURNAL.sqlx\n",
      "{'name': 'V_STG_SI_GLJOURNAL_L1', 'table_description': \"The SRC_SI_GLJOURNAL table stores metadata for general ledger (GL) journals, providing a comprehensive definition of each journal's attributes and configuration. Each record represents a unique GL journal, defining its parameters for posting and managing financial transactions.  It acts as a control mechanism for GL postings, linking to other tables such as SRC_SI_GLENTRY (for individual entries within the journal) and SRC_SI_GLBATCH (for batches of entries). The table includes details on the journal's effective dates, status, type (e.g., accrual, cash-basis), whether it's used for adjustments, and whether direct posting is allowed.  It also captures key identifiers for tracking and reconciliation, including journal symbols, titles, and status flags.  This table is crucial for managing GL transaction posting, auditing journal activity, and providing a comprehensive view of the financial data flow within the system.  It also supports financial reporting and compliance requirements by providing detailed journal metadata.\\n\\nExamples of use cases:\\n\\n1. Defining parameters for different types of GL journals (e.g., General Journal, Sales Journal, Cash Management Journal).\\n2. Controlling access and posting permissions for different journals.\\n3. Managing the lifecycle of journals (e.g., activation, deactivation, deletion).\\n4. Tracking journal usage and activity.\\n5. Supporting financial reporting and compliance requirements by providing detailed journal metadata.\\n6. Auditing GL transactions by linking journals to entries and batches.\\n7. Identifying journals used for specific purposes (e.g., adjustments, inter-company transactions).\\n8. Analyzing journal usage patterns and identifying potential bottlenecks.\", 'columns': [{'name': 'SYMBOL', 'description': \"Unique symbol or identifier for the journal (e.g., 'GJ' for General Journal). Used as the journal key in the UI and API.\"}, {'name': 'START_DATE', 'description': 'The earliest date when this journal can be used to post transactions. Acts like an effective start date for the journal.'}, {'name': 'LAST_DATE', 'description': 'The latest date when a transaction was posted to this journal. Represents the last active date for the journal.'}, {'name': 'ADJ', 'description': 'Indicates whether this journal is used for adjustments. If true, journal entries may affect closed periods or require special processing.'}, {'name': 'BOOK_TYPE', 'description': \"Type of accounting book this journal belongs to (e.g., 'Accrual', 'Cash'). Provides further categorization beyond the BOOKID.\"}, {'name': 'BOOK_ID', 'description': \"Identifier for the accounting book this journal belongs to. Links to a table defining book parameters (e.g., chart of accounts). Example: 'ACCRUAL' for an accrual-based accounting book.\"}, {'name': 'BILLABLE', 'description': 'Indicates whether transactions in this journal are considered billable (i.e., can be invoiced to customers).'}, {'name': 'DISABLE_DIRECT_POSTING', 'description': 'If true, direct posting to this journal is disabled—meaning entries must go through a batch process or approval workflow.'}, {'name': 'STATUS', 'description': \"Status of the journal—commonly values include 'active' or 'inactive'. Reflects whether the journal is available for transaction posting.\"}, {'name': 'RECORD_NO', 'description': 'Internal primary key for this journal record within DDS. Unique identifier within the database.'}, {'name': 'LAST_MODIFIED_DATE', 'description': 'Timestamp for the last modification to the journal.'}, {'name': 'CREATED_DATE', 'description': 'Timestamp for when the journal record was initially created.'}, {'name': 'CREATED_BY', 'description': 'Internal user ID indicating who created the journal record.'}, {'name': 'MODIFIED_BY', 'description': 'Internal user ID of who last modified the journal.'}, {'name': 'DDS_READ_TIME', 'description': 'Timestamp when the journal record was last read or exported via DDS. Useful for ETL audit and incremental syncing.'}, {'name': 'TITLE', 'description': \"Descriptive name of the journal (e.g., 'Expense Journal', 'Sales Journal').\"}, {'name': 'LOAD_DATE', 'description': 'The timestamp when the record was loaded into the data warehouse or BI environment.'}, {'name': 'DELETED_BY', 'description': 'Internal user ID that flagged or performed deletion of the journal (if deletion occurred).'}, {'name': 'WHEN_DELETED', 'description': 'Timestamp marking when the journal record was deleted or deactivated.'}, {'name': 'DDS_CHANGE_TYPE', 'description': \"Indicates the type of change tracked by DDS (e.g., 'I' = Inserted, 'U' = Updated, 'D' = Deleted). Useful for CDC.\"}, {'name': 'IS_DELETED', 'description': 'Flag to indicate whether the record is considered deleted (soft delete).'}]}\n",
      "Generated SQL for SRC_SI_GLJOURNAL:\n",
      "SELECT SYMBOL AS SYMBOL,\n",
      "       START_DATE AS START_DATE,\n",
      "       LAST_DATE AS LAST_DATE,\n",
      "       ADJ AS ADJ,\n",
      "       BOOKTYPE AS BOOK_TYPE,\n",
      "       BOOKID AS BOOK_ID,\n",
      "       BILLABLE AS BILLABLE,\n",
      "       DISABLEDIRECTPOSTING AS DISABLE_DIRECT_POSTING,\n",
      "       STATUS AS STATUS,\n",
      "       RECORDNO AS RECORD_NO,\n",
      "       WHENMODIFIED AS LAST_MODIFIED_DATE,\n",
      "       WHENCREATED AS CREATED_DATE,\n",
      "       CREATEDBY AS CREATED_BY,\n",
      "       MODIFIEDBY AS MODIFIED_BY,\n",
      "       DDSREADTIME AS DDS_READ_TIME,\n",
      "       TITLE AS TITLE,\n",
      "       LOAD_DATE AS LOAD_DATE,\n",
      "       DELETEDBY AS DELETED_BY,\n",
      "       WHENDELETED AS WHEN_DELETED,\n",
      "       DDSCHANGETYPE AS DDS_CHANGE_TYPE,\n",
      "       ISDELETED AS IS_DELETED\n",
      "FROM `dataplatr-sandbox.sageintact.SRC_SI_GLJOURNAL`\n",
      " SQL is valid.\n",
      "Folder definitions/L1 may already exist: 409 Directory already exists at definitions/L1\n",
      " Wrote definitions/L1/V_STG_SI_GLJOURNAL_L1_metadata.json into Dataform workspace\n",
      "Folder definitions/L1 may already exist: 409 Directory already exists at definitions/L1\n",
      " Wrote definitions/L1/V_STG_SI_GLJOURNAL_L1.sqlx into Dataform workspace\n"
     ]
    }
   ],
   "source": [
    "# create_all_standardized_views_dbt(session,TARGET_DB, SRC_SCHEMA, TARGET_SCHEMA,MODEL,git_repo_url, branch=branch,github_token=github_token,auto_push=True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bb85084-777d-4c6f-86fa-99b1eb315f1a",
   "metadata": {
    "codeCollapsed": false,
    "id": "gVfoJi6yBT7J",
    "language": "python",
    "name": "cell38"
   },
   "outputs": [],
   "source": [
    "# with open(\"/tmp/appRoot/dbt/snowflake_llm_project/models/generated/V_STG_SI_GLACCOUNT_L1.sql\", \"r\") as f:\n",
    "#     print(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a6c4a8f-0e41-4f7f-889e-e058ca23c129",
   "metadata": {
    "codeCollapsed": false,
    "id": "i4qXxEx26Gdj",
    "language": "python",
    "name": "cell39"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf4e1986-65a9-4e80-9dd3-b05fe9212f27",
   "metadata": {
    "id": "iI-3rAxk6GaW",
    "language": "python",
    "name": "cell40"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "ARabjRx50fG9",
    "11dy3iiorUyy",
    "ijZYWS2enAu3"
   ],
   "name": "L1_agent",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "lastEditStatus": {
   "authorEmail": "NITIGYA.KARGETI@DATAPLATR.COM",
   "authorId": "5674377515160",
   "authorName": "NITIGYA.KARGETI",
   "lastEditTime": 1760653969353,
   "notebookId": "i5w2n54th5g3ewflmvxv",
   "sessionId": "c70bd7aa-ae09-4f18-8600-e479d1089f2e"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
